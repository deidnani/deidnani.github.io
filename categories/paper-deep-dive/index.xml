<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Paper Deep Dive on Beyond the Buzz</title>
        <link>http://deidnani.github.io/categories/paper-deep-dive/</link>
        <description>Recent content in Paper Deep Dive on Beyond the Buzz</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 18 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://deidnani.github.io/categories/paper-deep-dive/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Transfer Learning for Visual Navigation</title>
        <link>http://deidnani.github.io/p/transfer-learning-for-visual-navigation/</link>
        <pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>http://deidnani.github.io/p/transfer-learning-for-visual-navigation/</guid>
        <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In the task of visual navigation, an agent (often a robot) has to move around in an environment to reach a goal, only by using its camera to explore the environment and accordingly avoid obstacles. This task is important in several applications such as to build service robots at home, search and rescue robots, assistive technology for the visually impaired, or AR applications. 
Many examples of this task exist such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PointNav: An agent is asked to go to a specific (x,y) location in the environment&lt;/li&gt;
&lt;li&gt;ObjectNav: The agent must find an object by its name and go to it&lt;/li&gt;
&lt;li&gt;RoomNav: The agent is asked to go to a specific room in the environment&lt;/li&gt;
&lt;li&gt;AudioNav: The agent must find a specific sound (such as a telephone that is ringing) and then go to it&lt;/li&gt;
&lt;li&gt;ImageNav: The agent must find where a specific image was taken and then go to it&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/object_nav.gif&#34;
	width=&#34;852&#34;
	height=&#34;480&#34;
	srcset=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/object_nav_hu4490c01c2a979b3438430d46338cb8ee_1006882_480x0_resize_box.gif 480w, http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/object_nav_hu4490c01c2a979b3438430d46338cb8ee_1006882_1024x0_resize_box.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: An example of one of the tasks in visual navigation called ObjectNav&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the past, researchers have developed models that are specific to each task, often trained with reinforcement learning. At a high level, reinforcement learning involves training an agent to observe the environment state and to take actions that will maximize its reward (which is often a function provided by the researcher). The model/algorithm that decides which action to take is often called a &lt;strong&gt;policy&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;problems-with-a-task-specific-approach&#34;&gt;Problems with a Task-Specific Approach&lt;/h2&gt;
&lt;p&gt;However, this task-specific approach is not ideal due to the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we are using such an approach, we need access to environments for each of these specific tasks which may not always be available. Even with the use of simulations and computer-generated, photorealistic environments that may be available, typically days/weeks are required using several GPUs to train the model.&lt;/li&gt;
&lt;li&gt;Such an approach is not generalizable to other tasks as it is unable to learn skills that are shared across all tasks.&lt;/li&gt;
&lt;li&gt;Some tasks such as ObjectNav require manual annotations such as object labels, which limits the amount of data that can be collected (and manual annotations introduce additional error in the process as well).&lt;/li&gt;
&lt;li&gt;Such models cannot be used in a &amp;ldquo;lifelong learning&amp;rdquo; setup, where an agent will face new tasks that it has not seen before and must be able to solve them with none or limited training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This work focuses on learning skills that can be shared across all these tasks. One example the authors provide is that if an agent finds a microwave for the ObjectNav task, then finding the kitchen should be easy as well for the RoomNav task. They propose a modular transfer learning approach for visual navigation that enables zero-shot experience learning. This essentially means that they train a single model that can be &amp;ldquo;transferred&amp;rdquo; to all these tasks (called &lt;strong&gt;transfer learning&lt;/strong&gt;) easily without requiring data for each individual task (called &lt;strong&gt;zero-shot experience learning&lt;/strong&gt;). Their model works in the following manner:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The model is trained to learn a general semantic search policy: specifically, they create a model to solve the &lt;strong&gt;image-goal task&lt;/strong&gt; which is essentially the ImageNav task presented above. The authors use this task rather than the other tasks listed above as it requires no annotations, and it is easy to produce a lot of data as we can just collect images and locations as the robot/agent is moving around the environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Joint goal embeddings&lt;/strong&gt; are created for each of the different tasks. In other words, beyond just creating representations for images that can be used for the ImageNav, we will try to use these representations to create representations for other tasks such as representations for object labels for ObjectNav, representations for sounds for AudioNav, etc.&lt;/li&gt;
&lt;li&gt;Directly use these embeddings for each of the different tasks either in a &lt;strong&gt;zero-shot&lt;/strong&gt; manner (where new training data is provided for the new task) or &lt;strong&gt;fine-tune&lt;/strong&gt; the model when limited data is provided (means use the new data to make the model &amp;ldquo;more accurate&amp;rdquo;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/introduction.png&#34;
	width=&#34;589&#34;
	height=&#34;514&#34;
	srcset=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/introduction_hu308f8ce61c65bf6fac4fdeeb8b5837ad_410999_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/introduction_hu308f8ce61c65bf6fac4fdeeb8b5837ad_410999_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;114&#34;
		data-flex-basis=&#34;275px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: A visual depicting the ability for the model presented to be used towards several different tasks such as finding a potted plant, water dripping, a bed based on its sketch, or the living room&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;deep-dive&#34;&gt;Deep Dive&lt;/h2&gt;
&lt;h3 id=&#34;semantic-search-policy-for-image-goals&#34;&gt;Semantic Search Policy for Image Goals&lt;/h3&gt;
&lt;h4 id=&#34;problem-description&#34;&gt;Problem Description&lt;/h4&gt;
&lt;p&gt;The ImageNav problem is specifically formulated as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The agent starts at a random position $p_0$ in an environment and is asked to find location $p_G$ given image $I_G$.&lt;/li&gt;
&lt;li&gt;At each time step, the agent receives an image $o_t$ in RGB and needs to choose an action out of the following possible actions: move forward, turn left, turn right, or stop so that it can reach the goal before $S$ time steps have passed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reward-function-view-reward&#34;&gt;Reward Function (View Reward)&lt;/h4&gt;
&lt;p&gt;An intuitive function would be to reward the agent for getting closer to the goal and to make sure that it stops within a specific distance $d_s$ of the goal by using the distance between its current position $p_t$ and the goal $p_G$. However, this reward doesn&amp;rsquo;t help the agent reach &lt;strong&gt;semantic understanding&lt;/strong&gt; as it doesn&amp;rsquo;t provide any reward/incentive for the agent being able to understand the image. For example, on the way to reaching a goal such as an oven, the current reward function would not penalize the agent if it turned around and looked at other objects such as a book.&lt;/p&gt;
&lt;p&gt;To solve this problem, the authors propose also rewarding the agent if it looks in the direction of the image $I_G$ when reaching the goal position $p_G$. 
$$ r_t = r_d(d_t, d_{t-1}) + [d_t \leq d_s] r_{\alpha} (\alpha_t, \alpha_{t-1}) - \gamma $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 1: Formula for the reward function used to train the semantic search policy&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let us look at each of the components of this reward in detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$r_d$ represents how much closer the agent is to the goal than it was at the previous time step in terms of distance&lt;/li&gt;
&lt;li&gt;$r_\alpha$ represents how close the current facing angle of the agent is to the camera angle of the goal image $I_g$ in radians&lt;/li&gt;
&lt;li&gt;$[d_t \leq d_s]$ is an example of what is called an &lt;strong&gt;indicator&lt;/strong&gt; function, which means that the reward $r_{\alpha}$ is only provided when the robot is close enough to the goal (when the distance from the goal $d_t$ is less than or equal to some parameter $d_s = 1$)&lt;/li&gt;
&lt;li&gt;$\gamma = 0.01$ represents &lt;strong&gt;slack&lt;/strong&gt; and encourages the agent to move towards the goal as quickly as possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final reward of 10 is provided  when the agent is within $d_s$ of the goal and $a_s$ radians from the camera angle of the goal image $I_g$ as shown below:&lt;/p&gt;
&lt;p&gt;$$ R_s = 5 \times ([d_t \leq d_s] + [d_t \leq d_s \text{ and } \alpha_t \leq \alpha_s]) $$ 
&lt;em&gt;Equation 2: Formula for the final reward given at the end of the training sequence&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;view-augmentations&#34;&gt;View Augmentations&lt;/h4&gt;
&lt;p&gt;In machine learning, &lt;strong&gt;augmentations&lt;/strong&gt; are essentially synthetic data points created in addition to the training data so that more data is available to train the model. Here, instead of using the same fixed goal image $I_G$ for each training sequence (or &lt;strong&gt;episode&lt;/strong&gt;), the authors sample a random camera angle to produce a new $I_G$ for each training episode. This helps the model focus more on the images provided rather than the goal position $p_G$ and will help the model learn which objects are close to each other in the environment without constructing a map for the environment.&lt;/p&gt;
&lt;p&gt;For example, suppose in one training episode, the agent was asked to find chairs and saw the chairs from the viewpoint of the kitchen. In a later training episode, the agent will be asked to find the same chairs but will now see them from the viewpoint of the rest of the dining table. The augmentations would help the agent understand that the dining table is close to the kitchen, which would help it in later training episodes.&lt;/p&gt;
&lt;h4 id=&#34;policy-training&#34;&gt;Policy Training&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;To train the policy, the authors use a model to represent the image observations at each timestep $o_t$ called the observation encoder and a separate model to represent the goal image $I_G$ called the image-goal encoder.&lt;/li&gt;
&lt;li&gt;As its state, the semantic search policy uses the current image observation $o_t$ as well as prior image observations $o_1&amp;hellip;o_{t-1}$ and then uses an &lt;strong&gt;actor-critic&lt;/strong&gt; setup to pick the best action $a_t$ to maximize its rewards (explained above). At a high level, in an actor-critic setup, the actor decides which action to take given the current state $s_t$ and the critic decides how good the action $a_t$ was and informs the actor how it should change to pick better actions.&lt;/li&gt;
&lt;li&gt;The algorithm used to train this policy is called &lt;strong&gt;Proximal Policy Optimization&lt;/strong&gt; (PPO). PPO focuses on trying to improve the policy as much as possible without taking &amp;ldquo;a step too far&amp;rdquo; leading to degradation in performance (more details can be found in the &amp;ldquo;Links&amp;rdquo; section below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/semantic_search_policy.png&#34;
	width=&#34;561&#34;
	height=&#34;286&#34;
	srcset=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/semantic_search_policy_hub386373bb7de54d16bcf59d552684bca_100981_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/semantic_search_policy_hub386373bb7de54d16bcf59d552684bca_100981_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;196&#34;
		data-flex-basis=&#34;470px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: An outline of architecture of the semantic search policy as well as the reward function used to train it&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;joint-goal-embedding-learning&#34;&gt;Joint Goal Embedding Learning&lt;/h3&gt;
&lt;p&gt;Our next goal is to use the image-goal encoder $f_G^I$ to help create representations for the goals presented for different tasks. Specifically, we want to create a embedding (or representation) space that understands relationships and similarities between the images that we trained the policy on and other representations of goals such as sketches, object labels, audio, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/joint_embedding.png&#34;
	width=&#34;396&#34;
	height=&#34;307&#34;
	srcset=&#34;http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/joint_embedding_hu46b17234ac2ad832adec87407353b133_48620_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/transfer-learning-for-visual-navigation/images/joint_embedding_hu46b17234ac2ad832adec87407353b133_48620_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;128&#34;
		data-flex-basis=&#34;309px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: A visual depicting the joint embedding space we are trying to create&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For example, let&amp;rsquo;s explore how we can use the image-goal encoder to create good representations for the object labels in the ObjectNav task. Note that this technique can be extended towards any other task: for example, for AudioNav, we will try to create good representations for the audio clips, rather than the object labels that we used for the ObjectNav task.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, for each of the object labels in the dataset, we need to add corresponding images. Only about 20K of these pairs are needed, which is much smaller than the amount of data points needed to train a task-specific model from scratch. Formally, we represent our dataset as image/goal pairs $D = {(x_i, g_i)}$ where $x_i$ is the goal images we used previously and $g_i$ are the object labels in this example.&lt;/li&gt;
&lt;li&gt;Next, we focus on creating the embedding space by minimizing the following loss function:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\mathcal{L}(x_i, g_i) = \begin{cases} 1 - \cos(f_G^I(x_i), f_G^M(g_i)) &amp;amp; y_i = 1 \\ \max(0, \cos(f_G^I(x_i), f_G^M(g_i))) &amp;amp; y_i = -1 \end{cases}$$
&lt;em&gt;Equation 3: Loss function used to craete the joint-goal embedding space&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the each of the components of this loss function in detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here, $f_G^M$ is the new goal encoder of modality $M$ that we are trying to create. In our example, the modality $M$ is text (as we are using the object labels) and $f_G^M$ will be used to create representations for the object labels that we will use to train a policy to solve the ObjectNav task.&lt;/li&gt;
&lt;li&gt;$\cos(\cdot,\cdot)$ is used to find the similarity between the representation of the goal image produced by the image-goal encoder and the representation of the object label produced by the modality-specific goal encoder $f_G^M$. For example, this will check how similar the representation for the label &amp;ldquo;chair&amp;rdquo; produced by $f_G^M$ is to the representation of the image of a chair produced by $f_G^I$.&lt;/li&gt;
&lt;li&gt;This loss function will be used to push these representations closer to each other (when $y_i = 1$) when the underlying object label and goal image are similar and push them further apart when the underlying object label and goal image are different. So, looking at the previous example, we want the repesentation for the label &amp;ldquo;chair&amp;rdquo; to be pushed toward the representation for the image of a chair but away from representations for images of a lamp, for example.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Now, all we have to do is swap out our image-goal encoder $f_G^I$ for the modality-specific goal encoder we constructed $f_G^M$ in the architecture of the semantic search policy outlined in Figure 3 and we can now solve the ObjectNav task as well!&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;image-goal-navigation&#34;&gt;Image-Goal Navigation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First, the authors evaluated the main task that the model was trained to solve: ImageNav. They found that the model outperforms previous baselines by a significant baseline, tending to be 6.6% more successful and taking 2.6% less steps to solve the task.&lt;/li&gt;
&lt;li&gt;The authors also find that removing the view reward or view augmentation led to degradations in performance the model worked best when both techniques worked together. Even under noisy environments, the model was able to solve the task better than other baselines.&lt;/li&gt;
&lt;li&gt;The authors also tested whether their model could be extended towards different image datasets with more diversity or with more complex, larger scenes. Although this did lead to a degradation in performance as expected, the authors found that the model could be extended towards these other datasets more effectively compared to the other baselines.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transfer-to-downstream-tasks&#34;&gt;Transfer to Downstream Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Next, the authors evaluated the model&amp;rsquo;s ability to transfer to other navigation tasks such as RoomNav, ObjectNav, and ViewNav.&lt;/li&gt;
&lt;li&gt;The authors found that their approach outperforms all other baselines by a significant margin and that methods that did not rely on a large amount of annotated data for the target task (for example, their approach) performed as well as the methods that did. Each episode also starts with a higher reward and improves faster, leading to the model being 12.5x faster than the fastest baseline.&lt;/li&gt;
&lt;li&gt;In addition, the authors find that training on ImageNav first and then transferring to other tasks leads to up to 15% gains in success across the tasks than training on other tasks first such as PointNav.&lt;/li&gt;
&lt;li&gt;Even in the zero-shot setting (when no data is provided for these tasks), the authors found that the model is surprisingly more effective than other models that were provided this data. Other approaches that also similarly try to transfer to these tasks and task-specific models are both able to reach a similar level of success but require much significantly more steps to train.&lt;/li&gt;
&lt;li&gt;The authors also test only transferring over the policy $\pi$ or the observation encoder $f_O$ or training them from scratch for the new task but find that the model learns best when both components are transferred and not trained from scratch.&lt;/li&gt;
&lt;li&gt;Finally, the authors find that the performance on the target task improves when more data is provided for the ImageNav task that the model is trained on.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Video-and-Language Pre-Training with AlPRO</title>
        <link>http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/</link>
        <pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/</guid>
        <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;A recent trend in AI is to solve tasks that use multiple modalities (text, audio, images, video, etc.). As part of this effort, one specific area is video-and-language understanding, which includes tasks such as text-video retrieval, video question answering, and video captioning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks.png&#34;
	width=&#34;660&#34;
	height=&#34;478&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks_hube1e6d1256efda1602d77307a85c0770_21189_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks_hube1e6d1256efda1602d77307a85c0770_21189_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;331px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Examples of video-language tasks from the VALUE (Video-and-Language Understanding Evaluation) paper&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since there are so many tasks in this field and new tasks will likely appear in the near future, researchers have worked to create unified models that can solve many of these tasks. One such approach to create a unified model is to develop a &lt;strong&gt;pretraining&lt;/strong&gt; strategy that develops meaningful representations for the video and text which can then be used for any of these tasks.&lt;/p&gt;
&lt;h2 id=&#34;difficulties-with-video-language-pre-training&#34;&gt;Difficulties with Video-Language Pre-Training&lt;/h2&gt;
&lt;p&gt;However, there are several difficulties with creating such a model, some of which are outlined below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compared to images, consecutive frames in videos may convey the same information or may be slightly different and we would be wasting memory and computation power if we use every single frame to solve a specific task (especially in an online system).&lt;/li&gt;
&lt;li&gt;The feature representations of videos and text are often in completely different vector spaces and understanding the interactions between these two is not as trivial, often leading to &lt;strong&gt;misalignment&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Tasks used to train these pre-trained models typically focus on the high-level information conveyed in a video rather than specific, &lt;strong&gt;fine-grained&lt;/strong&gt; information such as the objects shown.&lt;/li&gt;
&lt;li&gt;Videos also have the additional difficulty unlike images of conveying &lt;strong&gt;temporal&lt;/strong&gt; information: the easy solution of simply using the frames of the video independently of each other completely ignores this information. In other words, the model doesn&amp;rsquo;t understand that the video is a set of frames that take place after each other.&lt;/li&gt;
&lt;li&gt;Since annotation is expensive and sometimes subjective (especially in the case of video captioning), there is a lack of large datasets of data that are manually annotated, so approaches often need to use unlabeled data or create &lt;strong&gt;pseudo-labels&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this literature review, we will be looking at a papers that focus on solving these challenges called &lt;strong&gt;ALPRO&lt;/strong&gt; by Salesforce Research, which focuses on solving the misalignment problem through a &lt;em&gt;video-text contrastive&lt;/em&gt; (VTC) loss and &amp;ldquo;lack of fine-grained information&amp;rdquo; problem through a &lt;em&gt;prompting entity modeling&lt;/em&gt; (PEM) pretraining task.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;AlPRO first focuses on creating representations for the text and video independently (called &lt;strong&gt;unimodal encoding&lt;/strong&gt;) and then combining these representations with a multimodal encoder to capture the interactions between the two. To make sure that the representations of the video and text align well before being sent to the encoder, the model uses a video-text contrastive loss, which pushes the representations of the similar texts and video to be closer to each other and vice-versa. To capture fine-grained visual information, AlPRO uses a prompting entity modeling (PEM) pretraining objective, where the model is asked to predict the objects/nouns (called &lt;strong&gt;entities&lt;/strong&gt;) from random crops that are extracted from the video. Since the labels for these entities are not available, a &lt;em&gt;entity prompter&lt;/em&gt; module is added to create &amp;ldquo;pseudo-labels&amp;rdquo; as proxies for the true labels. These pseudo-labels are essentially similarity scores that it calculates between the random crops and a set of sentences (called &lt;strong&gt;prompts&lt;/strong&gt;) of the form &amp;ldquo;A video of {entity}&amp;rdquo;. These entities in its &amp;ldquo;dictionary&amp;rdquo; are sampled from nouns that were found in the pretraining data (called &lt;strong&gt;corpus&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example.png&#34;
	width=&#34;553&#34;
	height=&#34;595&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example_hu33cbd42a934ca63b69fe3e78b9e3124e_197182_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example_hu33cbd42a934ca63b69fe3e78b9e3124e_197182_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;223px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: An example of how AlPRO differs from previous approaches and how it uses the entity prompter module for its pretraining objective&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;deep-dive&#34;&gt;Deep Dive&lt;/h2&gt;
&lt;h3 id=&#34;encoding-the-video-and-text&#34;&gt;Encoding the Video and Text&lt;/h3&gt;
&lt;h4 id=&#34;visual-encoder&#34;&gt;Visual Encoder&lt;/h4&gt;
&lt;p&gt;For the visual encoder, a 12-layer &lt;strong&gt;TimeSFormer&lt;/strong&gt; is used which takes input frames of size $224 \times 224$. This model works in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each frame is split into $K$ patches which are then encoded to produce representations for each patch. For example, in the image below we can see that $K = 16$.&lt;/li&gt;
&lt;li&gt;Similar to standard transformers, &lt;strong&gt;positional embeddings&lt;/strong&gt; are also created so that the model is aware of the position of different patches in the image (since the position and content of the patch both convey important information). For example, the model needs to understand that the patch showing &amp;ldquo;fingers&amp;rdquo; is close to the patch showing &amp;ldquo;hand&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Temporal and spatial attention is then performed to create $K \times d$ features for each frame. The way this is done is shown below. Think of &lt;strong&gt;attention&lt;/strong&gt; as an &amp;ldquo;information sharing&amp;rdquo; mechanism between patches in the same frame or across frames where patches choose which other patches they want to receive information from. &lt;strong&gt;Space attention&lt;/strong&gt; means that information can only be shared between patches in the same frame and not across frames. &lt;strong&gt;Joint space-time attention&lt;/strong&gt; means that information can shared between every patch in every frame. However, to improve efficiency, &lt;strong&gt;Divided space-time attention&lt;/strong&gt;, which the TimeSformer uses, means that information can be shared between a particular patch and every patch in the same frame (just like space attention) but also the same patch in all other frames.&lt;/li&gt;
&lt;li&gt;The features are then averaged across frames to create a single $K \times d$ vector for the whole video. Other than this $K \times d$ vector, similar to other transformers, a representation for the [CLS] token is added which can receive information from all the patches in the video across all frames and thus can be seen as a single $d$-dimensional representation for the entire video.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention.png&#34;
	width=&#34;600&#34;
	height=&#34;531&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention_huf6edaf166425396bd3767ea35ccb2b49_319177_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention_huf6edaf166425396bd3767ea35ccb2b49_319177_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;271px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Example of the divided space-time attention that is used in the TimeSformer compared to standard space attention and attending across the entire space across time (joint space-time attention)&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;text-encoder&#34;&gt;Text Encoder&lt;/h4&gt;
&lt;p&gt;For text, a standard 6-layer transformer is used to create representations for the text from the first 6 layers of a pretrained text transformer known as $BERT_{base}$. The text is split into tokens and each token is encoded to produce a $d$-dimensional representation. Similar to the visual encoder, positional embeddings are added and the [CLS] token is used to contain information of the entire text sequence as it can receive information from all tokens in the text sequence.&lt;/p&gt;
&lt;h4 id=&#34;multimodal-encoder&#34;&gt;Multimodal Encoder&lt;/h4&gt;
&lt;p&gt;The multimodal encoder is a standard transformer that takes the representations of the video and text and aggregates them to create a combined representation. The video and text representations are directly concatenated to produce a $(N_v + N_t) \times d$ vector (where $N_v \times d$ was the size of the video representation and $N_t \times d$ was the size of the text representation) outputted by the multimodal encoder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders.png&#34;
	width=&#34;607&#34;
	height=&#34;318&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders_hu7aa39442e4f59237e884156c0d7eff22_53893_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders_hu7aa39442e4f59237e884156c0d7eff22_53893_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;458px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 4: An outline of how the video and text are encoded and fed into a multimodal video-text encoder&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;pretraining&#34;&gt;Pretraining&lt;/h3&gt;
&lt;p&gt;ALPRO is trained on four objectives, including two traditional ones such as &lt;strong&gt;masked language modeling&lt;/strong&gt; (MLM) and &lt;strong&gt;video-text matching&lt;/strong&gt; (VTM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In MLM, the model is asked to predict text tokens that are masked out; specifically, certain tokens in a sentence are replaced with [MASK] and the model is asked to predict what the original token was.&lt;/li&gt;
&lt;li&gt;In VTM, the model is asked whether a video and text description are matching or not. Here, the [CLS] token of the multimodal representation is used (as it is a $d$-dimensional vector that represents both the video and text). To get video-text pairs that aren&amp;rsquo;t matching (also known as &lt;strong&gt;negative samples&lt;/strong&gt;), random videos and pieces of text are sampled from the training data. Specifically, &lt;strong&gt;hard negatives&lt;/strong&gt; are sampled through several ways so that the model finds it challenging to distinguish positive video-text pairs from negative ones, rather than just randomly sampling videos and pieces of text where it is easy for the model to figure out that the video and text are not matching.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will now focus on the novel objectives introduced: the video-text contrastive (VTC) loss and prompting entity-modeling (PEM) losses.&lt;/p&gt;
&lt;h4 id=&#34;video-text-contrastive-loss-vtc&#34;&gt;Video-Text Contrastive Loss (VTC)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, [CLS] embeddings for both the video and text are projected to a lower-dimensional ($256$-dimensional) space rather than the high, $d$-dimensional representation that was used previously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, similarity scores are then calculated by taking the dot-product of both embeddings (Dot products typically measure how similar two vectors are).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The video-text contrastive loss consists of two terms, both of which use these similarity scores.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Firstly, a single video $V_i$ is compared to several pieces of text $T$ and we want to encourage the representation for $V_i$ to be similar to its most similar instance of text in $T$ which we will denote as $T_i$. Below, we can see that we&amp;rsquo;re comparing the similarity score of $(V_i, T_i)$ to the similarity scores for all other text $T_j$ in the batch through what is known as the &lt;strong&gt;softmax&lt;/strong&gt; function and then the logarithm is applied. $\tau$ is often known as the &lt;strong&gt;temperature&lt;/strong&gt; and scales the objective to better learn from the hard negatives.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{v2t} = -\log \frac{exp(s(V_i, T_i)/\tau)}{\sum_j exp(s(V_i, T_j)/\tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 1: First Component of Video-Text Contrastive Loss where a single video is compared to several pieces of text&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Similarly, a single piece of text $T_i$ is compared to several pieces of video $V$ and we want to encourage the representation for $T_i$ to be similar to its most similar instance of video in $V$ which we will denote as $V_i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{t2v} = -\log \frac{exp(s(T_i, V_i)/\tau)}{\sum_j exp(s(T_i, V_j) / \tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 2: Second Component of Video-Text Contrastive Loss where a single text is compared to several pieces of video&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;prompting-entity-modeling-pem-loss&#34;&gt;Prompting Entity-Modeling (PEM) Loss&lt;/h4&gt;
&lt;p&gt;Firstly, note that the entity prompter has a similar architecture to the video-language pre-training model without the multimodal video-text encoder as shown below. In other words, the visual and text encoders and the VTC loss are also used to create the entity prompter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter.png&#34;
	width=&#34;451&#34;
	height=&#34;177&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter_hu52a093c8ffd748f120964b726a419751_38581_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter_hu52a093c8ffd748f120964b726a419751_38581_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 5: An outline of how the video and text are encoded for the entity-prompter&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The entity prompter maintains a list of $M$ text entity prompts as explained above of the form &amp;ldquo;A video of {Entity}&amp;rdquo;. The entities are gathered by using a POS (part-of-speech) tagger and using the top 1,000 most frequent entities in the dataset.&lt;/li&gt;
&lt;li&gt;For each of these prompts, the [CLS] token is pre-computed to be used later.&lt;/li&gt;
&lt;li&gt;To generate the pseudo labels, we create random video crops $\hat{V}$ of the video and the corresponding [CLS] embedding using the visual encoder. Note that only crops that occupy 30-50% of the original spatial area of the frame are used.&lt;/li&gt;
&lt;li&gt;Next, we create the pseudo-label by finding the normalized similarity between the the crop [CLS] token $\hat{v_{cls}}$ and the prompt [CLS] tokens $t_{cls}$, with a formula similar to the contrastive-losses presented above. In other words, we are measuring the similarity between the random crop and the prompts in our &amp;ldquo;dictionary&amp;rdquo;. Note that only pseudo-labels that are above a certain threshold (0.2) are included in the calculations; in other words, we only include prompts that are &amp;ldquo;similar enough&amp;rdquo; to the crop that we have taken.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ q_{\hat{V}, m} = \frac{exp(s(\hat{V}, T_m) / \tau)}{\sum_m exp(s(\hat{V}, T_m) / \tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 3: Formula for the pseudo-labels for the entity prompter&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;During pre-training, we take the embeddings from the multimodal encoder corresponding to the specific video crop and average them to create a single $d$-dimensional vector and then feed them to a classifier to compute the entity prediction $p_{\hat{V}}$. This is done similarly as above except we don&amp;rsquo;t use the similarity scores calculated by the entity prompter but the embeddings directly produced by the multimodal encoder.&lt;/li&gt;
&lt;li&gt;Next, a &lt;strong&gt;cross-entropy&lt;/strong&gt; loss between $p_{\hat{V}}$ and $q_{\hat{V}}$ is used as our final loss. Note that the cross-entropy loss encourages the two distributions (and accordingly, vectors) $p$ and $q$ to be as similar as possible as the term grows larger when there&amp;rsquo;s an inconsistency between $p$ and $q$ (we predict a different label than the pseudo-label) and vice-versa. In other words, we want our video-language pre-training model to find the same prompts similar to a particular video crop as the entity prompter does, and this is accomplished through the loss function shown below.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{PEM} = -\sum_m q_{\hat{V}, m} \log p_{\hat{V}, m} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 4: PEM loss which is simply cross-entropy between the pseudo-labels and the video-text model&amp;rsquo;s predictions for the entities&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework.png&#34;
	width=&#34;1099&#34;
	height=&#34;439&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework_huea0b605ba29f72fa144a633967b775b4_112952_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework_huea0b605ba29f72fa144a633967b775b4_112952_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;600px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 6: An outline of the full framework&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;pre-training-data&#34;&gt;Pre-training Data&lt;/h3&gt;
&lt;p&gt;For pre-training data, the authors use WebVid-2M, which contains 2.5 million video-text pairs with static videos created by arranging duplicates from images in the CC-3M dataset into videos.&lt;/p&gt;
&lt;h3 id=&#34;experiment-data&#34;&gt;Experiment Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For text-video retrieval, the MSRVTT dataset is used which contains 10,000 videos with 200,000 text captions.&lt;/li&gt;
&lt;li&gt;For video-question answering, the MSVD-QA dataset is uised which contains 1,970 videos and 50,000 question answer pairs with 2,423 answer candidates and MSVRTT-QA which uses videos/captions from MSRVTT with 243,000 questions and 1,5000 answer candidates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The authors find that both losses improve performance across all tasks and pseudo-labels are diverse and meaningful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VTC is found to be important for the text-video retrieval task since it solves the alignment problem between videos and text.&lt;/li&gt;
&lt;li&gt;PEM helps for question-answering due to its ability to learn specific features in videos. However, performance is worse on MSVD-QA since MSVD-QA focuses on more region-level knowledge and only PEM plays a key role while MSRVTT-QA focuses on coarser-grained information so both losses play a key role.&lt;/li&gt;
&lt;li&gt;When compared to other approaches, the authors find that AlPRO surpasses other methods substantially with a &lt;strong&gt;6%&lt;/strong&gt; improvement in performance with video-text retrieval and on-par results for video question answering even though much &lt;strong&gt;less data&lt;/strong&gt; is used compared to other methods such as VQA-T which use 69 million QA pairs for training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels.png&#34;
	width=&#34;730&#34;
	height=&#34;418&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels_hua53b5db7787f0b95ee399cba88962330_337694_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels_hua53b5db7787f0b95ee399cba88962330_337694_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 7: Examples of the pseudo-labels produced for an image&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Through ablations, authors find using multiple prompts (or templates) other than just the single template of &amp;ldquo;A video of a {Entity}&amp;rdquo; such as &amp;ldquo;A footage of one {Entity}&amp;rdquo; leads to additional gains in performance.&lt;/li&gt;
&lt;li&gt;Additionally, keeping the number of entities low enough so that only the most frequent entities are added to the &amp;ldquo;dictionary&amp;rdquo; seems to play an important role as well.&lt;/li&gt;
&lt;li&gt;Keeping the number of frames low is also important as the authors find that increasing the number of frames up to 8 increases performance but limited gains are seen when the number of frames is increased beyond 8.&lt;/li&gt;
&lt;li&gt;Through these ablations and other analyses, the authors plan to focus more on creating better prompts as well as using temporal information to select the crops in the videos in the near future.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
