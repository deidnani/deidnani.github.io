<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>CV on Beyond the Buzz</title>
        <link>http://deidnani.github.io/tags/cv/</link>
        <description>Recent content in CV on Beyond the Buzz</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 11 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://deidnani.github.io/tags/cv/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Video-and-Language Pre-Training with AlPRO</title>
        <link>http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/</link>
        <pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/</guid>
        <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;A recent trend in AI is to solve tasks that use multiple modalities (text, audio, images, video, etc.). As part of this effort, one specific area is video-and-language understanding, which includes tasks such as text-video retrieval, video question answering, and video captioning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks.png&#34;
	width=&#34;660&#34;
	height=&#34;478&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks_hube1e6d1256efda1602d77307a85c0770_21189_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks_hube1e6d1256efda1602d77307a85c0770_21189_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;331px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Examples of video-language tasks from the VALUE (Video-and-Language Understanding Evaluation) paper&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since there are so many tasks in this field and new tasks will likely appear in the near future, researchers have worked to create unified models that can solve many of these tasks. One such approach to create a unified model is to develop a &lt;strong&gt;pretraining&lt;/strong&gt; strategy that develops meaningful representations for the video and text which can then be used for any of these tasks.&lt;/p&gt;
&lt;h2 id=&#34;difficulties-with-video-language-pre-training&#34;&gt;Difficulties with Video-Language Pre-Training&lt;/h2&gt;
&lt;p&gt;However, there are several difficulties with creating such a model, some of which are outlined below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compared to images, consecutive frames in videos may convey the same information or may be slightly different and we would be wasting memory and computation power if we use every single frame to solve a specific task (especially in an online system).&lt;/li&gt;
&lt;li&gt;The feature representations of videos and text are often in completely different vector spaces and understanding the interactions between these two is not as trivial, often leading to &lt;strong&gt;misalignment&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Tasks used to train these pre-trained models typically focus on the high-level information conveyed in a video rather than specific, &lt;strong&gt;fine-grained&lt;/strong&gt; information such as the objects shown.&lt;/li&gt;
&lt;li&gt;Videos also have the additional difficulty unlike images of conveying &lt;strong&gt;temporal&lt;/strong&gt; information: the easy solution of simply using the frames of the video independently of each other completely ignores this information. In other words, the model doesn&amp;rsquo;t understand that the video is a set of frames that take place after each other.&lt;/li&gt;
&lt;li&gt;Since annotation is expensive and sometimes subjective (especially in the case of video captioning), there is a lack of large datasets of data that are manually annotated, so approaches often need to use unlabeled data or create &lt;strong&gt;pseudo-labels&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this literature review, we will be looking at a papers that focus on solving these challenges called &lt;strong&gt;ALPRO&lt;/strong&gt; by Salesforce Research, which focuses on solving the misalignment problem through a &lt;em&gt;video-text contrastive&lt;/em&gt; (VTC) loss and &amp;ldquo;lack of fine-grained information&amp;rdquo; problem through a &lt;em&gt;prompting entity modeling&lt;/em&gt; (PEM) pretraining task.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;AlPRO first focuses on creating representations for the text and video independently (called &lt;strong&gt;unimodal encoding&lt;/strong&gt;) and then combining these representations with a multimodal encoder to capture the interactions between the two. To make sure that the representations of the video and text align well before being sent to the encoder, the model uses a video-text contrastive loss, which pushes the representations of the similar texts and video to be closer to each other and vice-versa. To capture fine-grained visual information, AlPRO uses a prompting entity modeling (PEM) pretraining objective, where the model is asked to predict the objects/nouns (called &lt;strong&gt;entities&lt;/strong&gt;) from random crops that are extracted from the video. Since the labels for these entities are not available, a &lt;em&gt;entity prompter&lt;/em&gt; module is added to create &amp;ldquo;pseudo-labels&amp;rdquo; as proxies for the true labels. These pseudo-labels are essentially similarity scores that it calculates between the random crops and a set of sentences (called &lt;strong&gt;prompts&lt;/strong&gt;) of the form &amp;ldquo;A video of {entity}&amp;rdquo;. These entities in its &amp;ldquo;dictionary&amp;rdquo; are sampled from nouns that were found in the pretraining data (called &lt;strong&gt;corpus&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example.png&#34;
	width=&#34;553&#34;
	height=&#34;595&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example_hu33cbd42a934ca63b69fe3e78b9e3124e_197182_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example_hu33cbd42a934ca63b69fe3e78b9e3124e_197182_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;223px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: An example of how AlPRO differs from previous approaches and how it uses the entity prompter module for its pretraining objective&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;deep-dive&#34;&gt;Deep Dive&lt;/h2&gt;
&lt;h3 id=&#34;encoding-the-video-and-text&#34;&gt;Encoding the Video and Text&lt;/h3&gt;
&lt;h4 id=&#34;visual-encoder&#34;&gt;Visual Encoder&lt;/h4&gt;
&lt;p&gt;For the visual encoder, a 12-layer &lt;strong&gt;TimeSFormer&lt;/strong&gt; is used which takes input frames of size $224 \times 224$. This model works in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each frame is split into $K$ patches which are then encoded to produce representations for each patch. For example, in the image below we can see that $K = 16$.&lt;/li&gt;
&lt;li&gt;Similar to standard transformers, &lt;strong&gt;positional embeddings&lt;/strong&gt; are also created so that the model is aware of the position of different patches in the image (since the position and content of the patch both convey important information). For example, the model needs to understand that the patch showing &amp;ldquo;fingers&amp;rdquo; is close to the patch showing &amp;ldquo;hand&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Temporal and spatial attention is then performed to create $K \times d$ features for each frame. The way this is done is shown below. Think of &lt;strong&gt;attention&lt;/strong&gt; as an &amp;ldquo;information sharing&amp;rdquo; mechanism between patches in the same frame or across frames where patches choose which other patches they want to receive information from. &lt;strong&gt;Space attention&lt;/strong&gt; means that information can only be shared between patches in the same frame and not across frames. &lt;strong&gt;Joint space-time attention&lt;/strong&gt; means that information can shared between every patch in every frame. However, to improve efficiency, &lt;strong&gt;Divided space-time attention&lt;/strong&gt;, which the TimeSformer uses, means that information can be shared between a particular patch and every patch in the same frame (just like space attention) but also the same patch in all other frames.&lt;/li&gt;
&lt;li&gt;The features are then averaged across frames to create a single $K \times d$ vector for the whole video. Other than this $K \times d$ vector, similar to other transformers, a representation for the [CLS] token is added which can receive information from all the patches in the video across all frames and thus can be seen as a single $d$-dimensional representation for the entire video.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention.png&#34;
	width=&#34;600&#34;
	height=&#34;531&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention_huf6edaf166425396bd3767ea35ccb2b49_319177_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention_huf6edaf166425396bd3767ea35ccb2b49_319177_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;271px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Example of the divided space-time attention that is used in the TimeSformer compared to standard space attention and attending across the entire space across time (joint space-time attention)&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;text-encoder&#34;&gt;Text Encoder&lt;/h4&gt;
&lt;p&gt;For text, a standard 6-layer transformer is used to create representations for the text from the first 6 layers of a pretrained text transformer known as $BERT_{base}$. The text is split into tokens and each token is encoded to produce a $d$-dimensional representation. Similar to the visual encoder, positional embeddings are added and the [CLS] token is used to contain information of the entire text sequence as it can receive information from all tokens in the text sequence.&lt;/p&gt;
&lt;h4 id=&#34;multimodal-encoder&#34;&gt;Multimodal Encoder&lt;/h4&gt;
&lt;p&gt;The multimodal encoder is a standard transformer that takes the representations of the video and text and aggregates them to create a combined representation. The video and text representations are directly concatenated to produce a $(N_v + N_t) \times d$ vector (where $N_v \times d$ was the size of the video representation and $N_t \times d$ was the size of the text representation) outputted by the multimodal encoder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders.png&#34;
	width=&#34;607&#34;
	height=&#34;318&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders_hu7aa39442e4f59237e884156c0d7eff22_53893_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders_hu7aa39442e4f59237e884156c0d7eff22_53893_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;458px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 4: An outline of how the video and text are encoded and fed into a multimodal video-text encoder&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;pretraining&#34;&gt;Pretraining&lt;/h3&gt;
&lt;p&gt;ALPRO is trained on four objectives, including two traditional ones such as &lt;strong&gt;masked language modeling&lt;/strong&gt; (MLM) and &lt;strong&gt;video-text matching&lt;/strong&gt; (VTM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In MLM, the model is asked to predict text tokens that are masked out; specifically, certain tokens in a sentence are replaced with [MASK] and the model is asked to predict what the original token was.&lt;/li&gt;
&lt;li&gt;In VTM, the model is asked whether a video and text description are matching or not. Here, the [CLS] token of the multimodal representation is used (as it is a $d$-dimensional vector that represents both the video and text). To get video-text pairs that aren&amp;rsquo;t matching (also known as &lt;strong&gt;negative samples&lt;/strong&gt;), random videos and pieces of text are sampled from the training data. Specifically, &lt;strong&gt;hard negatives&lt;/strong&gt; are sampled through several ways so that the model finds it challenging to distinguish positive video-text pairs from negative ones, rather than just randomly sampling videos and pieces of text where it is easy for the model to figure out that the video and text are not matching.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will now focus on the novel objectives introduced: the video-text contrastive (VTC) loss and prompting entity-modeling (PEM) losses.&lt;/p&gt;
&lt;h4 id=&#34;video-text-contrastive-loss-vtc&#34;&gt;Video-Text Contrastive Loss (VTC)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, [CLS] embeddings for both the video and text are projected to a lower-dimensional ($256$-dimensional) space rather than the high, $d$-dimensional representation that was used previously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, similarity scores are then calculated by taking the dot-product of both embeddings (Dot products typically measure how similar two vectors are).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The video-text contrastive loss consists of two terms, both of which use these similarity scores.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Firstly, a single video $V_i$ is compared to several pieces of text $T$ and we want to encourage the representation for $V_i$ to be similar to its most similar instance of text in $T$ which we will denote as $T_i$. Below, we can see that we&amp;rsquo;re comparing the similarity score of $(V_i, T_i)$ to the similarity scores for all other text $T_j$ in the batch through what is known as the &lt;strong&gt;softmax&lt;/strong&gt; function and then the logarithm is applied. $\tau$ is often known as the &lt;strong&gt;temperature&lt;/strong&gt; and scales the objective to better learn from the hard negatives.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{v2t} = -\log \frac{exp(s(V_i, T_i)/\tau)}{\sum_j exp(s(V_i, T_j)/\tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 1: First Component of Video-Text Contrastive Loss where a single video is compared to several pieces of text&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Similarly, a single piece of text $T_i$ is compared to several pieces of video $V$ and we want to encourage the representation for $T_i$ to be similar to its most similar instance of video in $V$ which we will denote as $V_i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{t2v} = -\log \frac{exp(s(T_i, V_i)/\tau)}{\sum_j exp(s(T_i, V_j) / \tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 2: Second Component of Video-Text Contrastive Loss where a single text is compared to several pieces of video&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;prompting-entity-modeling-pem-loss&#34;&gt;Prompting Entity-Modeling (PEM) Loss&lt;/h4&gt;
&lt;p&gt;Firstly, note that the entity prompter has a similar architecture to the video-language pre-training model without the multimodal video-text encoder as shown below. In other words, the visual and text encoders and the VTC loss are also used to create the entity prompter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter.png&#34;
	width=&#34;451&#34;
	height=&#34;177&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter_hu52a093c8ffd748f120964b726a419751_38581_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter_hu52a093c8ffd748f120964b726a419751_38581_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 5: An outline of how the video and text are encoded for the entity-prompter&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The entity prompter maintains a list of $M$ text entity prompts as explained above of the form &amp;ldquo;A video of {Entity}&amp;rdquo;. The entities are gathered by using a POS (part-of-speech) tagger and using the top 1,000 most frequent entities in the dataset.&lt;/li&gt;
&lt;li&gt;For each of these prompts, the [CLS] token is pre-computed to be used later.&lt;/li&gt;
&lt;li&gt;To generate the pseudo labels, we create random video crops $\hat{V}$ of the video and the corresponding [CLS] embedding using the visual encoder. Note that only crops that occupy 30-50% of the original spatial area of the frame are used.&lt;/li&gt;
&lt;li&gt;Next, we create the pseudo-label by finding the normalized similarity between the the crop [CLS] token $\hat{v_{cls}}$ and the prompt [CLS] tokens $t_{cls}$, with a formula similar to the contrastive-losses presented above. In other words, we are measuring the similarity between the random crop and the prompts in our &amp;ldquo;dictionary&amp;rdquo;. Note that only pseudo-labels that are above a certain threshold (0.2) are included in the calculations; in other words, we only include prompts that are &amp;ldquo;similar enough&amp;rdquo; to the crop that we have taken.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ q_{\hat{V}, m} = \frac{exp(s(\hat{V}, T_m) / \tau)}{\sum_m exp(s(\hat{V}, T_m) / \tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 3: Formula for the pseudo-labels for the entity prompter&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;During pre-training, we take the embeddings from the multimodal encoder corresponding to the specific video crop and average them to create a single $d$-dimensional vector and then feed them to a classifier to compute the entity prediction $p_{\hat{V}}$. This is done similarly as above except we don&amp;rsquo;t use the similarity scores calculated by the entity prompter but the embeddings directly produced by the multimodal encoder.&lt;/li&gt;
&lt;li&gt;Next, a &lt;strong&gt;cross-entropy&lt;/strong&gt; loss between $p_{\hat{V}}$ and $q_{\hat{V}}$ is used as our final loss. Note that the cross-entropy loss encourages the two distributions (and accordingly, vectors) $p$ and $q$ to be as similar as possible as the term grows larger when there&amp;rsquo;s an inconsistency between $p$ and $q$ (we predict a different label than the pseudo-label) and vice-versa. In other words, we want our video-language pre-training model to find the same prompts similar to a particular video crop as the entity prompter does, and this is accomplished through the loss function shown below.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{PEM} = -\sum_m q_{\hat{V}, m} \log p_{\hat{V}, m} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 4: PEM loss which is simply cross-entropy between the pseudo-labels and the video-text model&amp;rsquo;s predictions for the entities&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework.png&#34;
	width=&#34;1099&#34;
	height=&#34;439&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework_huea0b605ba29f72fa144a633967b775b4_112952_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework_huea0b605ba29f72fa144a633967b775b4_112952_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;600px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 6: An outline of the full framework&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;pre-training-data&#34;&gt;Pre-training Data&lt;/h3&gt;
&lt;p&gt;For pre-training data, the authors use WebVid-2M, which contains 2.5 million video-text pairs with static videos created by arranging duplicates from images in the CC-3M dataset into videos.&lt;/p&gt;
&lt;h3 id=&#34;experiment-data&#34;&gt;Experiment Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For text-video retrieval, the MSRVTT dataset is used which contains 10,000 videos with 200,000 text captions.&lt;/li&gt;
&lt;li&gt;For video-question answering, the MSVD-QA dataset is uised which contains 1,970 videos and 50,000 question answer pairs with 2,423 answer candidates and MSVRTT-QA which uses videos/captions from MSRVTT with 243,000 questions and 1,5000 answer candidates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The authors find that both losses improve performance across all tasks and pseudo-labels are diverse and meaningful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VTC is found to be important for the text-video retrieval task since it solves the alignment problem between videos and text.&lt;/li&gt;
&lt;li&gt;PEM helps for question-answering due to its ability to learn specific features in videos. However, performance is worse on MSVD-QA since MSVD-QA focuses on more region-level knowledge and only PEM plays a key role while MSRVTT-QA focuses on coarser-grained information so both losses play a key role.&lt;/li&gt;
&lt;li&gt;When compared to other approaches, the authors find that AlPRO surpasses other methods substantially with a &lt;strong&gt;6%&lt;/strong&gt; improvement in performance with video-text retrieval and on-par results for video question answering even though much &lt;strong&gt;less data&lt;/strong&gt; is used compared to other methods such as VQA-T which use 69 million QA pairs for training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels.png&#34;
	width=&#34;730&#34;
	height=&#34;418&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels_hua53b5db7787f0b95ee399cba88962330_337694_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels_hua53b5db7787f0b95ee399cba88962330_337694_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 7: Examples of the pseudo-labels produced for an image&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Through ablations, authors find using multiple prompts (or templates) other than just the single template of &amp;ldquo;A video of a {Entity}&amp;rdquo; such as &amp;ldquo;A footage of one {Entity}&amp;rdquo; leads to additional gains in performance.&lt;/li&gt;
&lt;li&gt;Additionally, keeping the number of entities low enough so that only the most frequent entities are added to the &amp;ldquo;dictionary&amp;rdquo; seems to play an important role as well.&lt;/li&gt;
&lt;li&gt;Keeping the number of frames low is also important as the authors find that increasing the number of frames up to 8 increases performance but limited gains are seen when the number of frames is increased beyond 8.&lt;/li&gt;
&lt;li&gt;Through these ablations and other analyses, the authors plan to focus more on creating better prompts as well as using temporal information to select the crops in the videos in the near future.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
