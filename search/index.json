[{"content":"Motivation In the task of visual navigation, an agent (often a robot) has to move around in an environment to reach a goal, only by using its camera to explore the environment and accordingly avoid obstacles. This task is important in several applications such as to build service robots at home, search and rescue robots, assistive technology for the visually impaired, or AR applications. Many examples of this task exist such as:\nPointNav: An agent is asked to go to a specific (x,y) location in the environment ObjectNav: The agent must find an object by its name and go to it RoomNav: The agent is asked to go to a specific room in the environment AudioNav: The agent must find a specific sound (such as a telephone that is ringing) and then go to it ImageNav: The agent must find where a specific image was taken and then go to it Figure 1: An example of one of the tasks in visual navigation called ObjectNav\nIn the past, researchers have developed models that are specific to each task, often trained with reinforcement learning. At a high level, reinforcement learning involves training an agent to observe the environment state and to take actions that will maximize its reward (which is often a function provided by the researcher). The model/algorithm that decides which action to take is often called a policy.\nProblems with a Task-Specific Approach However, this task-specific approach is not ideal due to the following reasons:\nIf we are using such an approach, we need access to environments for each of these specific tasks which may not always be available. Even with the use of simulations and computer-generated, photorealistic environments that may be available, typically days/weeks are required using several GPUs to train the model. Such an approach is not generalizable to other tasks as it is unable to learn skills that are shared across all tasks. Some tasks such as ObjectNav require manual annotations such as object labels, which limits the amount of data that can be collected (and manual annotations introduce additional error in the process as well). Such models cannot be used in a \u0026ldquo;lifelong learning\u0026rdquo; setup, where an agent will face new tasks that it has not seen before and must be able to solve them with none or limited training data. Introduction This work focuses on learning skills that can be shared across all these tasks. One example the authors provide is that if an agent finds a microwave for the ObjectNav task, then finding the kitchen should be easy as well for the RoomNav task. They propose a modular transfer learning approach for visual navigation that enables zero-shot experience learning. This essentially means that they train a single model that can be \u0026ldquo;transferred\u0026rdquo; to all these tasks (called transfer learning) easily without requiring data for each individual task (called zero-shot experience learning). Their model works in the following manner:\nThe model is trained to learn a general semantic search policy: specifically, they create a model to solve the image-goal task which is essentially the ImageNav task presented above. The authors use this task rather than the other tasks listed above as it requires no annotations, and it is easy to produce a lot of data as we can just collect images and locations as the robot/agent is moving around the environment. Joint goal embeddings are created for each of the different tasks. In other words, beyond just creating representations for images that can be used for the ImageNav, we will try to use these representations to create representations for other tasks such as representations for object labels for ObjectNav, representations for sounds for AudioNav, etc. Directly use these embeddings for each of the different tasks either in a zero-shot manner (where new training data is provided for the new task) or fine-tune the model when limited data is provided (means use the new data to make the model \u0026ldquo;more accurate\u0026rdquo;). Figure 2: A visual depicting the ability for the model presented to be used towards several different tasks such as finding a potted plant, water dripping, a bed based on its sketch, or the living room\nDeep Dive Semantic Search Policy for Image Goals Problem Description The ImageNav problem is specifically formulated as follows:\nThe agent starts at a random position $p_0$ in an environment and is asked to find location $p_G$ given image $I_G$. At each time step, the agent receives an image $o_t$ in RGB and needs to choose an action out of the following possible actions: move forward, turn left, turn right, or stop so that it can reach the goal before $S$ time steps have passed. Reward Function (View Reward) An intuitive function would be to reward the agent for getting closer to the goal and to make sure that it stops within a specific distance $d_s$ of the goal by using the distance between its current position $p_t$ and the goal $p_G$. However, this reward doesn\u0026rsquo;t help the agent reach semantic understanding as it doesn\u0026rsquo;t provide any reward/incentive for the agent being able to understand the image. For example, on the way to reaching a goal such as an oven, the current reward function would not penalize the agent if it turned around and looked at other objects such as a book.\nTo solve this problem, the authors propose also rewarding the agent if it looks in the direction of the image $I_G$ when reaching the goal position $p_G$. $$ r_t = r_d(d_t, d_{t-1}) + [d_t \\leq d_s] r_{\\alpha} (\\alpha_t, \\alpha_{t-1}) - \\gamma $$\nEquation 1: Formula for the reward function used to train the semantic search policy\nLet us look at each of the components of this reward in detail:\n$r_d$ represents how much closer the agent is to the goal than it was at the previous time step in terms of distance $r_\\alpha$ represents how close the current facing angle of the agent is to the camera angle of the goal image $I_g$ in radians $[d_t \\leq d_s]$ is an example of what is called an indicator function, which means that the reward $r_{\\alpha}$ is only provided when the robot is close enough to the goal (when the distance from the goal $d_t$ is less than or equal to some parameter $d_s = 1$) $\\gamma = 0.01$ represents slack and encourages the agent to move towards the goal as quickly as possible. The final reward of 10 is provided when the agent is within $d_s$ of the goal and $a_s$ radians from the camera angle of the goal image $I_g$ as shown below:\n$$ R_s = 5 \\times ([d_t \\leq d_s] + [d_t \\leq d_s \\text{ and } \\alpha_t \\leq \\alpha_s]) $$ Equation 2: Formula for the final reward given at the end of the training sequence\nView Augmentations In machine learning, augmentations are essentially synthetic data points created in addition to the training data so that more data is available to train the model. Here, instead of using the same fixed goal image $I_G$ for each training sequence (or episode), the authors sample a random camera angle to produce a new $I_G$ for each training episode. This helps the model focus more on the images provided rather than the goal position $p_G$ and will help the model learn which objects are close to each other in the environment without constructing a map for the environment.\nFor example, suppose in one training episode, the agent was asked to find chairs and saw the chairs from the viewpoint of the kitchen. In a later training episode, the agent will be asked to find the same chairs but will now see them from the viewpoint of the rest of the dining table. The augmentations would help the agent understand that the dining table is close to the kitchen, which would help it in later training episodes.\nPolicy Training To train the policy, the authors use a model to represent the image observations at each timestep $o_t$ called the observation encoder and a separate model to represent the goal image $I_G$ called the image-goal encoder. As its state, the semantic search policy uses the current image observation $o_t$ as well as prior image observations $o_1\u0026hellip;o_{t-1}$ and then uses an actor-critic setup to pick the best action $a_t$ to maximize its rewards (explained above). At a high level, in an actor-critic setup, the actor decides which action to take given the current state $s_t$ and the critic decides how good the action $a_t$ was and informs the actor how it should change to pick better actions. The algorithm used to train this policy is called Proximal Policy Optimization (PPO). PPO focuses on trying to improve the policy as much as possible without taking \u0026ldquo;a step too far\u0026rdquo; leading to degradation in performance (more details can be found in the \u0026ldquo;Links\u0026rdquo; section below). Figure 3: An outline of architecture of the semantic search policy as well as the reward function used to train it\nJoint Goal Embedding Learning Our next goal is to use the image-goal encoder $f_G^I$ to help create representations for the goals presented for different tasks. Specifically, we want to create a embedding (or representation) space that understands relationships and similarities between the images that we trained the policy on and other representations of goals such as sketches, object labels, audio, etc.\nFigure 3: A visual depicting the joint embedding space we are trying to create\nFor example, let\u0026rsquo;s explore how we can use the image-goal encoder to create good representations for the object labels in the ObjectNav task. Note that this technique can be extended towards any other task: for example, for AudioNav, we will try to create good representations for the audio clips, rather than the object labels that we used for the ObjectNav task.\nFirst, for each of the object labels in the dataset, we need to add corresponding images. Only about 20K of these pairs are needed, which is much smaller than the amount of data points needed to train a task-specific model from scratch. Formally, we represent our dataset as image/goal pairs $D = {(x_i, g_i)}$ where $x_i$ is the goal images we used previously and $g_i$ are the object labels in this example. Next, we focus on creating the embedding space by minimizing the following loss function: $$\\mathcal{L}(x_i, g_i) = \\begin{cases} 1 - \\cos(f_G^I(x_i), f_G^M(g_i)) \u0026amp; y_i = 1 \\\\ \\max(0, \\cos(f_G^I(x_i), f_G^M(g_i))) \u0026amp; y_i = -1 \\end{cases}$$ Equation 3: Loss function used to craete the joint-goal embedding space\nLet\u0026rsquo;s look at the each of the components of this loss function in detail:\nHere, $f_G^M$ is the new goal encoder of modality $M$ that we are trying to create. In our example, the modality $M$ is text (as we are using the object labels) and $f_G^M$ will be used to create representations for the object labels that we will use to train a policy to solve the ObjectNav task. $\\cos(\\cdot,\\cdot)$ is used to find the similarity between the representation of the goal image produced by the image-goal encoder and the representation of the object label produced by the modality-specific goal encoder $f_G^M$. For example, this will check how similar the representation for the label \u0026ldquo;chair\u0026rdquo; produced by $f_G^M$ is to the representation of the image of a chair produced by $f_G^I$. This loss function will be used to push these representations closer to each other (when $y_i = 1$) when the underlying object label and goal image are similar and push them further apart when the underlying object label and goal image are different. So, looking at the previous example, we want the repesentation for the label \u0026ldquo;chair\u0026rdquo; to be pushed toward the representation for the image of a chair but away from representations for images of a lamp, for example. Now, all we have to do is swap out our image-goal encoder $f_G^I$ for the modality-specific goal encoder we constructed $f_G^M$ in the architecture of the semantic search policy outlined in Figure 3 and we can now solve the ObjectNav task as well! Experiments Image-Goal Navigation First, the authors evaluated the main task that the model was trained to solve: ImageNav. They found that the model outperforms previous baselines by a significant baseline, tending to be 6.6% more successful and taking 2.6% less steps to solve the task. The authors also find that removing the view reward or view augmentation led to degradations in performance the model worked best when both techniques worked together. Even under noisy environments, the model was able to solve the task better than other baselines. The authors also tested whether their model could be extended towards different image datasets with more diversity or with more complex, larger scenes. Although this did lead to a degradation in performance as expected, the authors found that the model could be extended towards these other datasets more effectively compared to the other baselines. Transfer to Downstream Tasks Next, the authors evaluated the model\u0026rsquo;s ability to transfer to other navigation tasks such as RoomNav, ObjectNav, and ViewNav. The authors found that their approach outperforms all other baselines by a significant margin and that methods that did not rely on a large amount of annotated data for the target task (for example, their approach) performed as well as the methods that did. Each episode also starts with a higher reward and improves faster, leading to the model being 12.5x faster than the fastest baseline. In addition, the authors find that training on ImageNav first and then transferring to other tasks leads to up to 15% gains in success across the tasks than training on other tasks first such as PointNav. Even in the zero-shot setting (when no data is provided for these tasks), the authors found that the model is surprisingly more effective than other models that were provided this data. Other approaches that also similarly try to transfer to these tasks and task-specific models are both able to reach a similar level of success but require much significantly more steps to train. The authors also test only transferring over the policy $\\pi$ or the observation encoder $f_O$ or training them from scratch for the new task but find that the model learns best when both components are transferred and not trained from scratch. Finally, the authors find that the performance on the target task improves when more data is provided for the ImageNav task that the model is trained on. ","date":"2022-06-18T00:00:00Z","permalink":"http://deidnani.github.io/p/transfer-learning-for-visual-navigation/","title":"Transfer Learning for Visual Navigation"},{"content":"Motivation A recent trend in AI is to solve tasks that use multiple modalities (text, audio, images, video, etc.). As part of this effort, one specific area is video-and-language understanding, which includes tasks such as text-video retrieval, video question answering, and video captioning.\nFigure 1: Examples of video-language tasks from the VALUE (Video-and-Language Understanding Evaluation) paper\nSince there are so many tasks in this field and new tasks will likely appear in the near future, researchers have worked to create unified models that can solve many of these tasks. One such approach to create a unified model is to develop a pretraining strategy that develops meaningful representations for the video and text which can then be used for any of these tasks.\nDifficulties with Video-Language Pre-Training However, there are several difficulties with creating such a model, some of which are outlined below.\nCompared to images, consecutive frames in videos may convey the same information or may be slightly different and we would be wasting memory and computation power if we use every single frame to solve a specific task (especially in an online system). The feature representations of videos and text are often in completely different vector spaces and understanding the interactions between these two is not as trivial, often leading to misalignment. Tasks used to train these pre-trained models typically focus on the high-level information conveyed in a video rather than specific, fine-grained information such as the objects shown. Videos also have the additional difficulty unlike images of conveying temporal information: the easy solution of simply using the frames of the video independently of each other completely ignores this information. In other words, the model doesn\u0026rsquo;t understand that the video is a set of frames that take place after each other. Since annotation is expensive and sometimes subjective (especially in the case of video captioning), there is a lack of large datasets of data that are manually annotated, so approaches often need to use unlabeled data or create pseudo-labels. In this literature review, we will be looking at a papers that focus on solving these challenges called ALPRO by Salesforce Research, which focuses on solving the misalignment problem through a video-text contrastive (VTC) loss and \u0026ldquo;lack of fine-grained information\u0026rdquo; problem through a prompting entity modeling (PEM) pretraining task.\nIntroduction AlPRO first focuses on creating representations for the text and video independently (called unimodal encoding) and then combining these representations with a multimodal encoder to capture the interactions between the two. To make sure that the representations of the video and text align well before being sent to the encoder, the model uses a video-text contrastive loss, which pushes the representations of the similar texts and video to be closer to each other and vice-versa. To capture fine-grained visual information, AlPRO uses a prompting entity modeling (PEM) pretraining objective, where the model is asked to predict the objects/nouns (called entities) from random crops that are extracted from the video. Since the labels for these entities are not available, a entity prompter module is added to create \u0026ldquo;pseudo-labels\u0026rdquo; as proxies for the true labels. These pseudo-labels are essentially similarity scores that it calculates between the random crops and a set of sentences (called prompts) of the form \u0026ldquo;A video of {entity}\u0026rdquo;. These entities in its \u0026ldquo;dictionary\u0026rdquo; are sampled from nouns that were found in the pretraining data (called corpus).\nFigure 2: An example of how AlPRO differs from previous approaches and how it uses the entity prompter module for its pretraining objective\nDeep Dive Encoding the Video and Text Visual Encoder For the visual encoder, a 12-layer TimeSFormer is used which takes input frames of size $224 \\times 224$. This model works in the following way:\nEach frame is split into $K$ patches which are then encoded to produce representations for each patch. For example, in the image below we can see that $K = 16$. Similar to standard transformers, positional embeddings are also created so that the model is aware of the position of different patches in the image (since the position and content of the patch both convey important information). For example, the model needs to understand that the patch showing \u0026ldquo;fingers\u0026rdquo; is close to the patch showing \u0026ldquo;hand\u0026rdquo;. Temporal and spatial attention is then performed to create $K \\times d$ features for each frame. The way this is done is shown below. Think of attention as an \u0026ldquo;information sharing\u0026rdquo; mechanism between patches in the same frame or across frames where patches choose which other patches they want to receive information from. Space attention means that information can only be shared between patches in the same frame and not across frames. Joint space-time attention means that information can shared between every patch in every frame. However, to improve efficiency, Divided space-time attention, which the TimeSformer uses, means that information can be shared between a particular patch and every patch in the same frame (just like space attention) but also the same patch in all other frames. The features are then averaged across frames to create a single $K \\times d$ vector for the whole video. Other than this $K \\times d$ vector, similar to other transformers, a representation for the [CLS] token is added which can receive information from all the patches in the video across all frames and thus can be seen as a single $d$-dimensional representation for the entire video. Figure 3: Example of the divided space-time attention that is used in the TimeSformer compared to standard space attention and attending across the entire space across time (joint space-time attention)\nText Encoder For text, a standard 6-layer transformer is used to create representations for the text from the first 6 layers of a pretrained text transformer known as $BERT_{base}$. The text is split into tokens and each token is encoded to produce a $d$-dimensional representation. Similar to the visual encoder, positional embeddings are added and the [CLS] token is used to contain information of the entire text sequence as it can receive information from all tokens in the text sequence.\nMultimodal Encoder The multimodal encoder is a standard transformer that takes the representations of the video and text and aggregates them to create a combined representation. The video and text representations are directly concatenated to produce a $(N_v + N_t) \\times d$ vector (where $N_v \\times d$ was the size of the video representation and $N_t \\times d$ was the size of the text representation) outputted by the multimodal encoder.\nFigure 4: An outline of how the video and text are encoded and fed into a multimodal video-text encoder\nPretraining ALPRO is trained on four objectives, including two traditional ones such as masked language modeling (MLM) and video-text matching (VTM)\nIn MLM, the model is asked to predict text tokens that are masked out; specifically, certain tokens in a sentence are replaced with [MASK] and the model is asked to predict what the original token was. In VTM, the model is asked whether a video and text description are matching or not. Here, the [CLS] token of the multimodal representation is used (as it is a $d$-dimensional vector that represents both the video and text). To get video-text pairs that aren\u0026rsquo;t matching (also known as negative samples), random videos and pieces of text are sampled from the training data. Specifically, hard negatives are sampled through several ways so that the model finds it challenging to distinguish positive video-text pairs from negative ones, rather than just randomly sampling videos and pieces of text where it is easy for the model to figure out that the video and text are not matching. We will now focus on the novel objectives introduced: the video-text contrastive (VTC) loss and prompting entity-modeling (PEM) losses.\nVideo-Text Contrastive Loss (VTC) First, [CLS] embeddings for both the video and text are projected to a lower-dimensional ($256$-dimensional) space rather than the high, $d$-dimensional representation that was used previously.\nNext, similarity scores are then calculated by taking the dot-product of both embeddings (Dot products typically measure how similar two vectors are).\nThe video-text contrastive loss consists of two terms, both of which use these similarity scores.\nFirstly, a single video $V_i$ is compared to several pieces of text $T$ and we want to encourage the representation for $V_i$ to be similar to its most similar instance of text in $T$ which we will denote as $T_i$. Below, we can see that we\u0026rsquo;re comparing the similarity score of $(V_i, T_i)$ to the similarity scores for all other text $T_j$ in the batch through what is known as the softmax function and then the logarithm is applied. $\\tau$ is often known as the temperature and scales the objective to better learn from the hard negatives. $$ L_{v2t} = -\\log \\frac{exp(s(V_i, T_i)/\\tau)}{\\sum_j exp(s(V_i, T_j)/\\tau)} $$\nEquation 1: First Component of Video-Text Contrastive Loss where a single video is compared to several pieces of text\nSimilarly, a single piece of text $T_i$ is compared to several pieces of video $V$ and we want to encourage the representation for $T_i$ to be similar to its most similar instance of video in $V$ which we will denote as $V_i$. $$ L_{t2v} = -\\log \\frac{exp(s(T_i, V_i)/\\tau)}{\\sum_j exp(s(T_i, V_j) / \\tau)} $$\nEquation 2: Second Component of Video-Text Contrastive Loss where a single text is compared to several pieces of video\nPrompting Entity-Modeling (PEM) Loss Firstly, note that the entity prompter has a similar architecture to the video-language pre-training model without the multimodal video-text encoder as shown below. In other words, the visual and text encoders and the VTC loss are also used to create the entity prompter.\nFigure 5: An outline of how the video and text are encoded for the entity-prompter\nThe entity prompter maintains a list of $M$ text entity prompts as explained above of the form \u0026ldquo;A video of {Entity}\u0026rdquo;. The entities are gathered by using a POS (part-of-speech) tagger and using the top 1,000 most frequent entities in the dataset. For each of these prompts, the [CLS] token is pre-computed to be used later. To generate the pseudo labels, we create random video crops $\\hat{V}$ of the video and the corresponding [CLS] embedding using the visual encoder. Note that only crops that occupy 30-50% of the original spatial area of the frame are used. Next, we create the pseudo-label by finding the normalized similarity between the the crop [CLS] token $\\hat{v_{cls}}$ and the prompt [CLS] tokens $t_{cls}$, with a formula similar to the contrastive-losses presented above. In other words, we are measuring the similarity between the random crop and the prompts in our \u0026ldquo;dictionary\u0026rdquo;. Note that only pseudo-labels that are above a certain threshold (0.2) are included in the calculations; in other words, we only include prompts that are \u0026ldquo;similar enough\u0026rdquo; to the crop that we have taken. $$ q_{\\hat{V}, m} = \\frac{exp(s(\\hat{V}, T_m) / \\tau)}{\\sum_m exp(s(\\hat{V}, T_m) / \\tau)} $$\nEquation 3: Formula for the pseudo-labels for the entity prompter\nDuring pre-training, we take the embeddings from the multimodal encoder corresponding to the specific video crop and average them to create a single $d$-dimensional vector and then feed them to a classifier to compute the entity prediction $p_{\\hat{V}}$. This is done similarly as above except we don\u0026rsquo;t use the similarity scores calculated by the entity prompter but the embeddings directly produced by the multimodal encoder. Next, a cross-entropy loss between $p_{\\hat{V}}$ and $q_{\\hat{V}}$ is used as our final loss. Note that the cross-entropy loss encourages the two distributions (and accordingly, vectors) $p$ and $q$ to be as similar as possible as the term grows larger when there\u0026rsquo;s an inconsistency between $p$ and $q$ (we predict a different label than the pseudo-label) and vice-versa. In other words, we want our video-language pre-training model to find the same prompts similar to a particular video crop as the entity prompter does, and this is accomplished through the loss function shown below. $$ L_{PEM} = -\\sum_m q_{\\hat{V}, m} \\log p_{\\hat{V}, m} $$\nEquation 4: PEM loss which is simply cross-entropy between the pseudo-labels and the video-text model\u0026rsquo;s predictions for the entities\nFigure 6: An outline of the full framework\nExperiments Pre-training Data For pre-training data, the authors use WebVid-2M, which contains 2.5 million video-text pairs with static videos created by arranging duplicates from images in the CC-3M dataset into videos.\nExperiment Data For text-video retrieval, the MSRVTT dataset is used which contains 10,000 videos with 200,000 text captions. For video-question answering, the MSVD-QA dataset is uised which contains 1,970 videos and 50,000 question answer pairs with 2,423 answer candidates and MSVRTT-QA which uses videos/captions from MSRVTT with 243,000 questions and 1,5000 answer candidates. Results The authors find that both losses improve performance across all tasks and pseudo-labels are diverse and meaningful:\nVTC is found to be important for the text-video retrieval task since it solves the alignment problem between videos and text. PEM helps for question-answering due to its ability to learn specific features in videos. However, performance is worse on MSVD-QA since MSVD-QA focuses on more region-level knowledge and only PEM plays a key role while MSRVTT-QA focuses on coarser-grained information so both losses play a key role. When compared to other approaches, the authors find that AlPRO surpasses other methods substantially with a 6% improvement in performance with video-text retrieval and on-par results for video question answering even though much less data is used compared to other methods such as VQA-T which use 69 million QA pairs for training. Figure 7: Examples of the pseudo-labels produced for an image\nAblations Through ablations, authors find using multiple prompts (or templates) other than just the single template of \u0026ldquo;A video of a {Entity}\u0026rdquo; such as \u0026ldquo;A footage of one {Entity}\u0026rdquo; leads to additional gains in performance. Additionally, keeping the number of entities low enough so that only the most frequent entities are added to the \u0026ldquo;dictionary\u0026rdquo; seems to play an important role as well. Keeping the number of frames low is also important as the authors find that increasing the number of frames up to 8 increases performance but limited gains are seen when the number of frames is increased beyond 8. Through these ablations and other analyses, the authors plan to focus more on creating better prompts as well as using temporal information to select the crops in the videos in the near future. ","date":"2022-06-11T00:00:00Z","permalink":"http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/","title":"Video-and-Language Pre-Training with AlPRO"},{"content":"Motivation Why do we want to use language? Integrating Natural Language into robotic tasks can provide added convenience and interactivity for humans. For example, it can enable robotic tasks such as language-based navigation and object retrieval in which the robot is given prompts like \u0026ldquo;Get me a clean cup from the kitchen\u0026rdquo; and the robot must act accordingly.\nFigure 1: An example of how a robot using language and current perception to produce an action ($a_t$)\nFurthermore, language enables us to provide both high-level and low-level guidance. For example, rather than just telling the robot \u0026ldquo;Get me a clean cup from the kitchen\u0026rdquo; we can also tell it to \u0026ldquo;Go straight, then take a right, then walk straight to the kitchen counter and pick up the cup.\u0026rdquo;\nAdditionally, language is information-rich since it adds flexibility. The user can decide how much information it wants to give and the agent must infer using the provided information along with the data it has already seen so far.\nFinally, several tasks require language inputs. In addition to the language-based navigation application shown above, other tasks include translation, chat-based guidance, and question answering. Figure 2: An example of visual question answering, which combines computer vision with natural language processing.\nIntroduction To apply natural language to various tasks in computer vision and robotics, we must first ground the input. Grounding refers to mapping language-based inputs to robot or agent behavior. Grounding language to objects in the domain, actions the robot must take, and agent rewards are common examples of tasks revolving around this concept. However, grounding is an inherently hard problem since it involves understanding the underlying meaning of the language input. Since grounding is central to the success of language-based robots, it is often challenging to achieve high performance on language-based tasks. Figure 3: An example of how grounding can be used to convert action phrases to executable actions.\nRepresentation To train our models on language, we must first convert the language input into a representation the computer can understand. The first step in this process is breaking the sentence so that the model can learn about the correlation between sentences in our dataset.\nSequence of Tokens We often use a representation called sequence of tokens to represent each sentence where token refers to each element in the vocabulary used by the model. Hence, the size of the vocabulary is depended upon the tokenization we choose to represent our inputs with. Some common choices include tokenization by individual words, by characters, and by Byte-Pair Encodings (BPE). For the sentence Fetch me the vacuum cleaner:\n\u0026lt;Fetch\u0026gt;\u0026lt; \u0026gt;\u0026lt;me\u0026gt;\u0026lt; \u0026gt;\u0026lt;the\u0026gt;\u0026lt; \u0026gt;\u0026lt;vacuum\u0026gt;\u0026lt; \u0026gt;\u0026lt;cleaner\u0026gt; corresponds to the tokenization by individual words.\n\u0026lt;F\u0026gt;\u0026lt;e\u0026gt;\u0026lt;t\u0026gt;\u0026lt;c\u0026gt;\u0026lt;h\u0026gt;\u0026lt; \u0026gt;\u0026lt;m\u0026gt;\u0026lt;e\u0026gt;\u0026lt; \u0026gt;\u0026lt;t\u0026gt;\u0026lt;h\u0026gt;\u0026lt;e\u0026gt;\u0026lt; \u0026gt;\u0026lt;v\u0026gt;\u0026lt;a\u0026gt;\u0026lt;c\u0026gt;\u0026lt;u\u0026gt;\u0026lt;u\u0026gt;\u0026lt;m\u0026gt;\u0026lt; \u0026gt;\u0026lt;c\u0026gt;\u0026lt;l\u0026gt;\u0026lt;e\u0026gt;\u0026lt;a\u0026gt;\u0026lt;e\u0026gt;\u0026lt;r\u0026gt; corresponds to the tokenization by individual characters.\n\u0026lt;Fet\u0026gt;\u0026lt;ch\u0026gt;\u0026lt; \u0026gt;\u0026lt;me\u0026gt;\u0026lt; \u0026gt;\u0026lt;the\u0026gt;\u0026lt; \u0026gt;\u0026lt;va\u0026gt;\u0026lt;cu\u0026gt;\u0026lt;um\u0026gt;\u0026lt; \u0026gt;\u0026lt;clean\u0026gt;\u0026lt;er\u0026gt; correponds to the tokenization by Byte-Pair encodings.\nByte-Pair Encodings have been discovered to be the perfect middle ground between tokenization by characters and by words. The biggest advantage of using BPE is the breakdown of rare words into common tokens so that the model can still understand the meaning of the word by combing those tokens. The tokens used for BPE can either be derived from a predefined vocabulary such as the ones used by popular models today or can be created using the following algorithm:\n1 2 3 4 1. Initialize vocabulary with individual characters and sentences with character tokenization. 2. Iteratively count the frequency of each token pair in the current sentence tokenization. 3. Merge every occurrence of the most frequent pair and the merged pair to the vocabulary. 4. Repeat steps 2 and 3 till the desired vocabulary size is achieved. The next step in representing sentences is converting each token into an array of integers and/or floats which can be input into our model. The four common strategies for achieving this representation are dictionary indices, Word2Vec/GloVe, RNN/LSTM, and Transformers.\nDictionary Indices Using dictionary indices is the easiest way of converting tokens into an array of floats. First, we order the words in our dictionary from 0 to the size of the dictionary - 1. Then, we use a one-hot vector of length = size of dictionary to represent each token, where all elements are $0$ except the value at the token\u0026rsquo;s index in the dictionary, which contains a $1$. For example, if a dictionary contains 2000 elements and if the token Machine is at the 342nd index in the dictionary, then its encoding will be an array of length 2000 containing 0s at all places except 1 at the 342nd index.\nWord2Vec and GloVe A one-hot vector for each token is impractical because our dictionary can have thousands of tokens. Word2Vec and GloVe are two common ways of converting one-hot vector representation of tokens to a smaller dimensional vector. Word2Vec refers to a group of shallow $2$-layer neural nets that can be used to create these smaller dimensional vectors. The neural nets can either use a continuous bag-of-words architecture or a continuous skip-gram architecture. Both models have a hidden layer that can be used after training to get word embedding. The continuous bag-of-words architecture refers to a feed-forward model that uses surrounding words to predict the next word. The skip-gram architecture on the other hand uses the current word to predict surrounding words. Figure 4: Bag-of-Words and Skip-Gram architectures used for Word2Vec\nGloVe also known as Global Vectors is an unsupervised algorithm for obtaining smaller dimensional vector representations. Global word-word co-occurrence statistics are first extracted from a training corpus and then a log-bilinear regression model is trained. The regression model combines global matrix factorization and local context window methods. Figure 5: Comparison of GloVe embeddings on different words. Values are normalized between $1$ (blue) and $-1$ (red).\nOne of the most important qualities of both embeddings is that they preserve similarity between different sentences that have the same meaning.\nRNN/LSTM Long-Short Term Memory (LSTM) models, which are a special type of Recurrent Neural Networks, can also be used to create vector representation of tokens. Additionally, they can also be used to create a vector representation of the whole sentence. Figure 6: A diagram of the internal structure of an LSTM cell. The model typically consists of several such cells with either weight shared. During training, the outputs from the current cell become the input into the next cell.\nThe hidden vector $h_t$ corresponds to the vector representation of each token while the last context vector $c_t$ corresponds to the vector representation of the whole sentence after training. LSTMs for vector representations are typically trained in an end-to-end fashion alongside the primary task.\nTransformer The Transformer is the current state-of-the-art model not only for encoding sentences but also grounding language in general. Transformers are particularly powerful because of multi-head self-attention which allows them to weigh different subsets of the input simultaneously. Figure 7: Model Architecture of the Transformer\nToday, Transformers are being applied to a variety of tasks, from NLP-oriented translation and text summarization tasks to multimodal objectives such as visual question answering and navigation. Transformers can be easily modified to take in multimodal inputs by modifying the input and output embedding layers to process images using CNNs. Image embeddings can then be concatenated with token embeddings to get a frame-by-frame multimodal embedding of the task.\nUnifying task-language data One example of how language data can help in executing a certain task was outlined in the Action2Vec framework that combined language data with videos. The framework did the following:\nCreate embeddings from video frames using a CNN Embed features extracted from CNN into a single vector using an LSTM Similarly, for words, create embeddings from words using Word2Vec and embed features extracted into a single vector using an LSTM or a pooling operation (avg/max pooling) Use a Pairwise Ranking Loss that takes these dense vectors and makes sure that words and videos that share the same concepts have similar embeddings and different words and videos have different embeddings Once, these embeddings are created, we can perform \u0026ldquo;arithmetic\u0026rdquo; on these embeddings. For example below, we take the embeddings for frames of a person playing piano and we try to create a video that shows a person playing a \u0026ldquo;violin\u0026rdquo; instead of a \u0026ldquo;piano\u0026rdquo;. This can be done by getting the mean Action2Vec embeddings for videos corresponding to piano playing, subtracting the Word2Vec embedding for \u0026ldquo;piano\u0026rdquo;, adding the Word2Vec embedding for \u0026ldquo;violin\u0026rdquo;, and searching the Action2Vec embedding space for all frames that are similar to the resulting vector. Figure 8: Model Architecture of the Action2Vec Framework Structured language approaches What is structured language? Structured language involves directly mapping words to concepts that the robot already knows. This is done by parsing the word sequence and tagging each word with its semantic purpose. These parsed sequences are mapped to pre-made logical primitives that the robot can interpret. One way these logical primitives can be represented is through linear temporal logic (LTL) specification. Also, this structured language approach requires a decent amount of overhead since the robot already needs to completely know all the actions available to it as well as the direct mapping from words to those actions.\nAs shown in figure 9, an example approach involves:\nTagging and parsing - Assigning a hierarchical structure to a sentence Null element restoration - Allows the structure of imperatives to be directly matched by a semantic interpretation module VerbNet frame matching - Identifies verbs and their arguments in parse trees using VerbNet LTL Formula Generation - Maps to existing logical primitive Figure 9: Conversion of the sentence \u0026ldquo;Go to the hallway\u0026rdquo; into LTL formulas through tagging, parsing, null element restoration, semantic interpretation, and LTL formula generation\nAlthough expensive to develop, this structure allows humans to directly inspect generated plans. This means that the system has guarantees and is verifiable. It is also simple to diagnose failures since researchers can investigate contradictions in planned behaviors.\nAdvantages \u0026amp; Disadvantages Advantages Generalization: It is possible to ensure that commands map directly to task executions Interpretability: Researches can directly analyze task executions to the language since it is human-readable Failure analysis: It is easy to debug since one can directly trace out the plans and language commands to resolve issues Disadvantages Brittle: The wording for the actions has to be very exact meaning the researcher need to memorize the wording for all the actions Expertise Required: Non-experts may have difficulties creating logic primitives and connecting them to corresponding sentence parses Ambiguity: The speaker needs to be exact in wording and not give under-specified commands (ex. “leave” instead of “leave the room”) Low-level language-task alignment Policy Sketches Policy sketches involve having low-level planners for each specific task and then having a high-level model that activates these sub-components to complete the full action. This includes training an overarching network that can understand a full task specification and then will use existing sub-policies when needed. Figure 10: The figure shows an example approach where simplified versions of two tasks (make planks and make sticks) and a model overview of the sub-policies\nAs shown in figure 10, the subtasks (ex. $b_1$ which is \u0026ldquo;get wood\u0026rdquo; ) are represented by a unique policy ($\\pi_i$); however, the high-level planner can active these components when necessary to make a full plan. In this approach, the high-level model deals with mapping language to sub-policies, and the low-level planners deal with executing action necessary for the sub-task. By using a more hierarchal approach, this approach avoids having to design a single end-to-end network deal with the entire process of converting language to desired actions.\nAdvantages \u0026amp; Disadvantages Advantages Generalization: It is possible to recombine existing policies to produce new behaviors Learned task models: Since it uses a learning approach, there is less developmental overhead and can be less brittle than structured language Differentiable: Able to learn task execution without needing an expert to specify logical plans Disadvantages Brittle: Policies are still triggered with pre-specified words used during training Plans required: The developers must create new behaviors and know how to combine explicit words to produce behaviors Reward Functions One way to reduce interaction time with the environment is to use reward shaping by designing a reward function that provides the agent intermediate rewards for progress towards the goal. Designing a good reward function can be very difficult, expensive, and time-consuming; however, one way to address this problem is by using language instructions to perform reward shaping. This reward function approach mainly involves mapping language to state and action in the world to determine if they are related. This can be done using a framework like LEARN which maps language to rewards based on the agent\u0026rsquo;s actions.\nFigure 11: Example framework which involves training LEARN module and using its intermediate rewards to assist within the agent-environment loop.\nFigure 11 shows an example pipeline of producing a reward function from labeled random trajectories. Its main steps involve:\nGather a dataset of labeled trajectories (sequence of past actions) Align trajectory information to language commands or prompts Basically, the labelers can explain what they see or say the commands that would have led to that behavior. (\u0026ldquo;Jump over the skull while going left\u0026rdquo;) Learn to associate the collected language commands and trajectories (ex. LEARN) If the resultant trajectory correctly corresponds to the language sequence, a positive reward is given (Language reward) Advantages \u0026amp; Disadvantages Advantages Generalization: Learning from human-created language prompts. Provide new utterances and map them to new trajectories/plans. Single model: Do not need to activate relevant policies One agent deals with learning the entire mapping Grounding is implicit: Do not need to manually label the environment or actions The model automatically learns the relationship between words to actions Disadvantages Data collection: Have to collect a LOT of labeled data for learning It is also difficult for a human to guide the training afterward Training time: Whenever new behavior with language needs to be added, the model needs to be fine-tuned before the behavior can be produced. Single model: Learning for multiple, unrelated tasks might be extremely difficult High-level language-task alignment What are High-level Tasks? In high-level language-task alignment, we focus on giving the agent more complex and \u0026ldquo;abstract\u0026rdquo; instructions, such as \u0026ldquo;go to the library\u0026rdquo;, compared to policy sketches which would use commands such as \u0026ldquo;go forwards, go left, go forwards, go right\u0026rdquo; where we have to explicitly lay out the subtasks for the agent. Note that such high-level tasks usually consist of a sequence of simple actions (known as primitives) or these actions associated in some hierarchical manner. Unlike the methods above, the agent, here, has to learn subtasks that need to be performed to complete the high-level task and has to learn how to complete each subtask.\nImitating Interactive Intelligence We look at Imitating Interactive Intelligence, a paper published by DeepMind in 2020 to better understand how agents can learn high-level language-task alignment. The paper involved two agents in a simulated bedroom as the environment that tried to learn from the demonstrations provided:\none that would provide tasks to be performed (known as the setter) The setter would create a language command based on the interaction and prompt provided by the environment. The interaction was the type of general task to be performed (ex: Instruction, Q\u0026amp;A). The prompt was used by the environment to specify the type of interaction to be performed (ex: for Q\u0026amp;A, the prompt specified whether the question should ask the count, color, location, or position of an object) one that would perform the task created by the setter (known as the solver). Figure 12: Relationship between Interactions, Prompts, and Instructions Created How are the agents trained? General Process of Training Gather human demonstrations Apply Behavior Cloning to demonstrations and learn policy from learned reward model through Forward RL to create agent policies Apply GAIL to create a reward model from the policy created and demonstrations provided Repeat steps 2. and 3. until the solver can complete tasks that are created by the setter. Using Behavior Cloning (BC) to Create Policies First, the authors took the image data from the simulated environment and passed it through a ResNet (a popular CNN architecture to understand images) to create the image embeddings. Similarly, the authors took text data from the prompts provided by the environment, previous language policies generated, and communication between the agents and they tokenized it to create textual embeddings. Next, the authors combined these embeddings and passed them through a multimodal transformer, as transformers have been proven to work well for vision-and-language tasks (ex: VILBERT, CLIP, etc). The outputs of the transformer would be lastly passed to an LSTM which would produce an autoregressive sequence of actions (meaning that actions depend on previous actions) that are needed to be performed to complete the task as well as any words to output. These policies are trained to match the demonstrator\u0026rsquo;s policies via Behavior Cloning. Figure 13: Model Architecture for using BC to Create Policies from Image and Language Data Using Inverse Reinforcement Learning (IRL) to create Reward Model Note that the Reward model similarly receives its inputs to the Policy model above: by using image and text data, converting these modalities of data into embeddings, fusing these embeddings, and using a multimodal transformer to understand the fused embeddings. The discriminator loss is used similarly as GAIL: to determine whether the trajectories were produced by the agent or if they were sampled from the demonstrations produced by experts. As GAIL is difficult to use with high-dimensional data, the authors also use multiple strategies outlined below to make GAIL effective for the setup chosen such as random image augmentation (ex: cropping, rotating, translating images), language matching, and object-in-view. The authors employed language matching as an auxiliary task for representation learning that checks whether the instruction created by the setter and the images inputted come from the same episode or not. This would require the ResNet to produce embeddings that have a high degree of mutual information with the tokenized inputs. The authors employed object-in-view as another auxiliary task for representation learning that asks the agent to predict whether a certain object is in the image data provided to the agent or not. This requires the ResNet to produce embeddings that reflect the objects present in the image data. Figure 14: Model Architecture for using IRL to Create Reward Models from Image and Language Data Results In study A below, the authors found that using behavior cloning and GAIL was more effective than simply learning behavior cloning and not learning the reward model for tasks that required following instructions. However, using GAIL to learn a reward model did not seem to significantly boost performance for question-answering tasks. This is because GAIL can help the agent understand what a task truly means, which is helpful for instruction following, while behavior cloning can not. On the other hand, behavior cloning can simply mimic the mapping from questions to answers that the demonstrator uses so it tends to be as effective as using behavior cloning and GAIL for Q\u0026amp;A tasks. In study B below, the authors also found that for a given task, such as finding the count of a certain object in a room, using data for other tasks for training, such as finding the position/color of an object, led to better performance over limiting the training data to only include demonstrations that directly corresponded to the task that was to be performed. This is because more diverse data and seemingly unrelated commands allow tasks to inform each other (due to some crossover), allows the agent to better understand the language of the commands it receives, and how this language is grounded in the environment. Study C shows that this phenomenon also helps with data efficiency; when trained with all types of prompts, the model only needed 1/8th as much data as training with only the \u0026ldquo;Position\u0026rdquo; prompt to reach the same level of performance. Study D similarly shows object-color generalization capabilities as the model was able to sufficiently perform tasks dealing with orange ducks such as \u0026ldquo;lift an orange duck\u0026rdquo; even when all instances of orange ducks were removed from the dataset. Figure 15: Graphs for some of the studies performed by the authors (described in detail above) Advantages \u0026amp; Disadvantages Advantages Generalization: Language commands learned are general and compositional Large Multi-Task Model: Agents do not need to learn sub-policies. They can simply learn a set of behaviors that can be used for a variety of tasks These behaviors also inform each other, leading to better performance for any task when compared to using a single-task approach Production of Language: Agents are not only able to understand the language commands but are also able to produce them Such an ability may allow the agent to explain itself Disadvantages Data Collection and Complex Training Strategies: This paper required 2+ years of data collection in a complex simulator that required 2 demonstrators interacting. Due to the expensiveness of the data collection, this paper is rather difficult to reproduce. Also, the complex tricks required may mean that the results are limited to the environment that the authors have set up and the constrained vocabulary (~550 unique words) that they are using Independent Training: Agent cannot actively ask for help on new problems. For example, the agent cannot ask to see more data on tasks that it has not been asked to perform before or on objects that it has not seen before Cannot Quickly Acquire New Knowledge: Since many data points are required and a high amount of compute is necessary for training, it is difficult for the model to learn new tasks or to gain new insights quickly, as this would require more data points for those tasks and the model would have to be re-trained from scratch. In other words, it is \u0026ldquo;set\u0026rdquo;. Future Research Directions Methods to learn from language that is representative of experiences in the \u0026ldquo;real world\u0026rdquo;, rather than using language that is crawled from the Internet Ground language in task execution and objects in the environment and use language is taken directly from humans, rather than just using tweets from Twitter or posts on Reddit Learning to use language for Active Learning Creating capabilities for the agent to ask for assistance, clarification, or to remove ambiguities using language, rather than just using offline learning that often requires many data points Creating efficient systems of NLP + RL Creating a model that can efficiently learn new tasks and knowledge beyond its existing knowledge base in an efficient manner Also, agents must learn to think and act quickly to be used in real-world situations where language instructions are given constantly Learning interactively Creating capabilities for the agent to ask for more demonstrations on a certain task or other forms of assistance so that the agent is more involved in the learning process (more like \u0026ldquo;situated learning\u0026rdquo;) Using Audio Data Audio data has previously not been used due to its high dimensionality. However, acoustics can influence the meaning of commands; for example, giving a command in a sarcastic tone is quite different from giving it in an urgent tone Also, the volume and duration of the acoustics could indicate whether the agent is receiving the command or whether the command was intended for someone else (allows the agent to learn from noisy commands, which typically occurs in the real world) Key Takeaways Using language can help solve agents solve many tasks but it remains difficult due to the problem of grounding the input. There are several representations we can use for language such as sequences of tokens, dictionary indices, Word2Vec, GloVe, RNNs, LSTMs, and Transformers, with each representation having its advantages and disadvantages. To unify task-learning and language for LfD, there are three main approaches we discussed: structure language approaches, low-level language-task alignment, and high-level language-task alignment (note that each approach is more complex yet more applicable in the real world than the previous). Leveraging natural language is still an open area of research with current approaches trying to leverage audio or trying to make these approaches more interactive, efficient, and applicable in the real world. ","date":"2022-06-01T00:00:00Z","permalink":"http://deidnani.github.io/p/leveraging-natural-language/","title":"Leveraging Natural Language"}]