[{"content":"Motivation Why do we want to use language? Integrating Natural Language into robotic tasks can provide added convenience and interactivity for humans. For example, it can enable robotic tasks such as language-based navigation and object retrieval in which the robot is given prompts like \u0026ldquo;Get me a clean cup from the kitchen\u0026rdquo; and the robot must act accordingly.\nFigure 1: An example of how a robot using language and current perception to produce an action ($a_t$)\nFurthermore, language enables us to provide both high-level and low-level guidance. For example, rather than just telling the robot \u0026ldquo;Get me a clean cup from the kitchen\u0026rdquo; we can also tell it to \u0026ldquo;Go straight, then take a right, then walk straight to the kitchen counter and pick up the cup.\u0026rdquo;\nAdditionally, language is information-rich since it adds flexibility. The user can decide how much information it wants to give and the agent must infer using the provided information along with the data it has already seen so far.\nFinally, several tasks require language inputs. In addition to the language-based navigation application shown above, other tasks include translation, chat-based guidance, and question answering. Figure 2: An example of visual question answering, which combines computer vision with natural language processing.\nIntroduction To apply natural language to various tasks in computer vision and robotics, we must first ground the input. Grounding refers to mapping language-based inputs to robot or agent behavior. Grounding language to objects in the domain, actions the robot must take, and agent rewards are common examples of tasks revolving around this concept. However, grounding is an inherently hard problem since it involves understanding the underlying meaning of the language input. Since grounding is central to the success of language-based robots, it is often challenging to achieve high performance on language-based tasks. Figure 3: An example of how grounding can be used to convert action phrases to executable actions.\nRepresentation To train our models on language, we must first convert the language input into a representation the computer can understand. The first step in this process is breaking the sentence so that the model can learn about the correlation between sentences in our dataset.\nSequence of Tokens We often use a representation called sequence of tokens to represent each sentence where token refers to each element in the vocabulary used by the model. Hence, the size of the vocabulary is depended upon the tokenization we choose to represent our inputs with. Some common choices include tokenization by individual words, by characters, and by Byte-Pair Encodings (BPE). For the sentence Fetch me the vacuum cleaner:\n\u0026lt;Fetch\u0026gt;\u0026lt; \u0026gt;\u0026lt;me\u0026gt;\u0026lt; \u0026gt;\u0026lt;the\u0026gt;\u0026lt; \u0026gt;\u0026lt;vacuum\u0026gt;\u0026lt; \u0026gt;\u0026lt;cleaner\u0026gt; corresponds to the tokenization by individual words.\n\u0026lt;F\u0026gt;\u0026lt;e\u0026gt;\u0026lt;t\u0026gt;\u0026lt;c\u0026gt;\u0026lt;h\u0026gt;\u0026lt; \u0026gt;\u0026lt;m\u0026gt;\u0026lt;e\u0026gt;\u0026lt; \u0026gt;\u0026lt;t\u0026gt;\u0026lt;h\u0026gt;\u0026lt;e\u0026gt;\u0026lt; \u0026gt;\u0026lt;v\u0026gt;\u0026lt;a\u0026gt;\u0026lt;c\u0026gt;\u0026lt;u\u0026gt;\u0026lt;u\u0026gt;\u0026lt;m\u0026gt;\u0026lt; \u0026gt;\u0026lt;c\u0026gt;\u0026lt;l\u0026gt;\u0026lt;e\u0026gt;\u0026lt;a\u0026gt;\u0026lt;e\u0026gt;\u0026lt;r\u0026gt; corresponds to the tokenization by individual characters.\n\u0026lt;Fet\u0026gt;\u0026lt;ch\u0026gt;\u0026lt; \u0026gt;\u0026lt;me\u0026gt;\u0026lt; \u0026gt;\u0026lt;the\u0026gt;\u0026lt; \u0026gt;\u0026lt;va\u0026gt;\u0026lt;cu\u0026gt;\u0026lt;um\u0026gt;\u0026lt; \u0026gt;\u0026lt;clean\u0026gt;\u0026lt;er\u0026gt; correponds to the tokenization by Byte-Pair encodings.\nByte-Pair Encodings have been discovered to be the perfect middle ground between tokenization by characters and by words. The biggest advantage of using BPE is the breakdown of rare words into common tokens so that the model can still understand the meaning of the word by combing those tokens. The tokens used for BPE can either be derived from a predefined vocabulary such as the ones used by popular models today or can be created using the following algorithm:\n1 2 3 4 1. Initialize vocabulary with individual characters and sentences with character tokenization. 2. Iteratively count the frequency of each token pair in the current sentence tokenization. 3. Merge every occurrence of the most frequent pair and the merged pair to the vocabulary. 4. Repeat steps 2 and 3 till the desired vocabulary size is achieved. The next step in representing sentences is converting each token into an array of integers and/or floats which can be input into our model. The four common strategies for achieving this representation are dictionary indices, Word2Vec/GloVe, RNN/LSTM, and Transformers.\nDictionary Indices Using dictionary indices is the easiest way of converting tokens into an array of floats. First, we order the words in our dictionary from 0 to the size of the dictionary - 1. Then, we use a one-hot vector of length = size of dictionary to represent each token, where all elements are $0$ except the value at the token\u0026rsquo;s index in the dictionary, which contains a $1$. For example, if a dictionary contains 2000 elements and if the token Machine is at the 342nd index in the dictionary, then its encoding will be an array of length 2000 containing 0s at all places except 1 at the 342nd index.\nWord2Vec and GloVe A one-hot vector for each token is impractical because our dictionary can have thousands of tokens. Word2Vec and GloVe are two common ways of converting one-hot vector representation of tokens to a smaller dimensional vector. Word2Vec refers to a group of shallow $2$-layer neural nets that can be used to create these smaller dimensional vectors. The neural nets can either use a continuous bag-of-words architecture or a continuous skip-gram architecture. Both models have a hidden layer that can be used after training to get word embedding. The continuous bag-of-words architecture refers to a feed-forward model that uses surrounding words to predict the next word. The skip-gram architecture on the other hand uses the current word to predict surrounding words. Figure 4: Bag-of-Words and Skip-Gram architectures used for Word2Vec\nGloVe also known as Global Vectors is an unsupervised algorithm for obtaining smaller dimensional vector representations. Global word-word co-occurrence statistics are first extracted from a training corpus and then a log-bilinear regression model is trained. The regression model combines global matrix factorization and local context window methods. Figure 5: Comparison of GloVe embeddings on different words. Values are normalized between $1$ (blue) and $-1$ (red).\nOne of the most important qualities of both embeddings is that they preserve similarity between different sentences that have the same meaning.\nRNN/LSTM Long-Short Term Memory (LSTM) models, which are a special type of Recurrent Neural Networks, can also be used to create vector representation of tokens. Additionally, they can also be used to create a vector representation of the whole sentence. Figure 6: A diagram of the internal structure of an LSTM cell. The model typically consists of several such cells with either weight shared. During training, the outputs from the current cell become the input into the next cell.\nThe hidden vector $h_t$ corresponds to the vector representation of each token while the last context vector $c_t$ corresponds to the vector representation of the whole sentence after training. LSTMs for vector representations are typically trained in an end-to-end fashion alongside the primary task.\nTransformer The Transformer is the current state-of-the-art model not only for encoding sentences but also grounding language in general. Transformers are particularly powerful because of multi-head self-attention which allows them to weigh different subsets of the input simultaneously. Figure 7: Model Architecture of the Transformer\nToday, Transformers are being applied to a variety of tasks, from NLP-oriented translation and text summarization tasks to multimodal objectives such as visual question answering and navigation. Transformers can be easily modified to take in multimodal inputs by modifying the input and output embedding layers to process images using CNNs. Image embeddings can then be concatenated with token embeddings to get a frame-by-frame multimodal embedding of the task.\nUnifying task-language data One example of how language data can help in executing a certain task was outlined in the Action2Vec framework that combined language data with videos. The framework did the following:\nCreate embeddings from video frames using a CNN Embed features extracted from CNN into a single vector using an LSTM Similarly, for words, create embeddings from words using Word2Vec and embed features extracted into a single vector using an LSTM or a pooling operation (avg/max pooling) Use a Pairwise Ranking Loss that takes these dense vectors and makes sure that words and videos that share the same concepts have similar embeddings and different words and videos have different embeddings Once, these embeddings are created, we can perform \u0026ldquo;arithmetic\u0026rdquo; on these embeddings. For example below, we take the embeddings for frames of a person playing piano and we try to create a video that shows a person playing a \u0026ldquo;violin\u0026rdquo; instead of a \u0026ldquo;piano\u0026rdquo;. This can be done by getting the mean Action2Vec embeddings for videos corresponding to piano playing, subtracting the Word2Vec embedding for \u0026ldquo;piano\u0026rdquo;, adding the Word2Vec embedding for \u0026ldquo;violin\u0026rdquo;, and searching the Action2Vec embedding space for all frames that are similar to the resulting vector. Figure 8: Model Architecture of the Action2Vec Framework Structured language approaches What is structured language? Structured language involves directly mapping words to concepts that the robot already knows. This is done by parsing the word sequence and tagging each word with its semantic purpose. These parsed sequences are mapped to pre-made logical primitives that the robot can interpret. One way these logical primitives can be represented is through linear temporal logic (LTL) specification. Also, this structured language approach requires a decent amount of overhead since the robot already needs to completely know all the actions available to it as well as the direct mapping from words to those actions.\nAs shown in figure 9, an example approach involves:\nTagging and parsing - Assigning a hierarchical structure to a sentence Null element restoration - Allows the structure of imperatives to be directly matched by a semantic interpretation module VerbNet frame matching - Identifies verbs and their arguments in parse trees using VerbNet LTL Formula Generation - Maps to existing logical primitive Figure 9: Conversion of the sentence \u0026ldquo;Go to the hallway\u0026rdquo; into LTL formulas through tagging, parsing, null element restoration, semantic interpretation, and LTL formula generation\nAlthough expensive to develop, this structure allows humans to directly inspect generated plans. This means that the system has guarantees and is verifiable. It is also simple to diagnose failures since researchers can investigate contradictions in planned behaviors.\nAdvantages \u0026amp; Disadvantages Advantages Generalization: It is possible to ensure that commands map directly to task executions Interpretability: Researches can directly analyze task executions to the language since it is human-readable Failure analysis: It is easy to debug since one can directly trace out the plans and language commands to resolve issues Disadvantages Brittle: The wording for the actions has to be very exact meaning the researcher need to memorize the wording for all the actions Expertise Required: Non-experts may have difficulties creating logic primitives and connecting them to corresponding sentence parses Ambiguity: The speaker needs to be exact in wording and not give under-specified commands (ex. “leave” instead of “leave the room”) Low-level language-task alignment Policy Sketches Policy sketches involve having low-level planners for each specific task and then having a high-level model that activates these sub-components to complete the full action. This includes training an overarching network that can understand a full task specification and then will use existing sub-policies when needed. Figure 10: The figure shows an example approach where simplified versions of two tasks (make planks and make sticks) and a model overview of the sub-policies\nAs shown in figure 10, the subtasks (ex. $b_1$ which is \u0026ldquo;get wood\u0026rdquo; ) are represented by a unique policy ($\\pi_i$); however, the high-level planner can active these components when necessary to make a full plan. In this approach, the high-level model deals with mapping language to sub-policies, and the low-level planners deal with executing action necessary for the sub-task. By using a more hierarchal approach, this approach avoids having to design a single end-to-end network deal with the entire process of converting language to desired actions.\nAdvantages \u0026amp; Disadvantages Advantages Generalization: It is possible to recombine existing policies to produce new behaviors Learned task models: Since it uses a learning approach, there is less developmental overhead and can be less brittle than structured language Differentiable: Able to learn task execution without needing an expert to specify logical plans Disadvantages Brittle: Policies are still triggered with pre-specified words used during training Plans required: The developers must create new behaviors and know how to combine explicit words to produce behaviors Reward Functions One way to reduce interaction time with the environment is to use reward shaping by designing a reward function that provides the agent intermediate rewards for progress towards the goal. Designing a good reward function can be very difficult, expensive, and time-consuming; however, one way to address this problem is by using language instructions to perform reward shaping. This reward function approach mainly involves mapping language to state and action in the world to determine if they are related. This can be done using a framework like LEARN which maps language to rewards based on the agent\u0026rsquo;s actions.\nFigure 11: Example framework which involves training LEARN module and using its intermediate rewards to assist within the agent-environment loop.\nFigure 11 shows an example pipeline of producing a reward function from labeled random trajectories. Its main steps involve:\nGather a dataset of labeled trajectories (sequence of past actions) Align trajectory information to language commands or prompts Basically, the labelers can explain what they see or say the commands that would have led to that behavior. (\u0026ldquo;Jump over the skull while going left\u0026rdquo;) Learn to associate the collected language commands and trajectories (ex. LEARN) If the resultant trajectory correctly corresponds to the language sequence, a positive reward is given (Language reward) Advantages \u0026amp; Disadvantages Advantages Generalization: Learning from human-created language prompts. Provide new utterances and map them to new trajectories/plans. Single model: Do not need to activate relevant policies One agent deals with learning the entire mapping Grounding is implicit: Do not need to manually label the environment or actions The model automatically learns the relationship between words to actions Disadvantages Data collection: Have to collect a LOT of labeled data for learning It is also difficult for a human to guide the training afterward Training time: Whenever new behavior with language needs to be added, the model needs to be fine-tuned before the behavior can be produced. Single model: Learning for multiple, unrelated tasks might be extremely difficult High-level language-task alignment What are High-level Tasks? In high-level language-task alignment, we focus on giving the agent more complex and \u0026ldquo;abstract\u0026rdquo; instructions, such as \u0026ldquo;go to the library\u0026rdquo;, compared to policy sketches which would use commands such as \u0026ldquo;go forwards, go left, go forwards, go right\u0026rdquo; where we have to explicitly lay out the subtasks for the agent. Note that such high-level tasks usually consist of a sequence of simple actions (known as primitives) or these actions associated in some hierarchical manner. Unlike the methods above, the agent, here, has to learn subtasks that need to be performed to complete the high-level task and has to learn how to complete each subtask.\nImitating Interactive Intelligence We look at Imitating Interactive Intelligence, a paper published by DeepMind in 2020 to better understand how agents can learn high-level language-task alignment. The paper involved two agents in a simulated bedroom as the environment that tried to learn from the demonstrations provided:\none that would provide tasks to be performed (known as the setter) The setter would create a language command based on the interaction and prompt provided by the environment. The interaction was the type of general task to be performed (ex: Instruction, Q\u0026amp;A). The prompt was used by the environment to specify the type of interaction to be performed (ex: for Q\u0026amp;A, the prompt specified whether the question should ask the count, color, location, or position of an object) one that would perform the task created by the setter (known as the solver). Figure 12: Relationship between Interactions, Prompts, and Instructions Created How are the agents trained? General Process of Training Gather human demonstrations Apply Behavior Cloning to demonstrations and learn policy from learned reward model through Forward RL to create agent policies Apply GAIL to create a reward model from the policy created and demonstrations provided Repeat steps 2. and 3. until the solver can complete tasks that are created by the setter. Using Behavior Cloning (BC) to Create Policies First, the authors took the image data from the simulated environment and passed it through a ResNet (a popular CNN architecture to understand images) to create the image embeddings. Similarly, the authors took text data from the prompts provided by the environment, previous language policies generated, and communication between the agents and they tokenized it to create textual embeddings. Next, the authors combined these embeddings and passed them through a multimodal transformer, as transformers have been proven to work well for vision-and-language tasks (ex: VILBERT, CLIP, etc). The outputs of the transformer would be lastly passed to an LSTM which would produce an autoregressive sequence of actions (meaning that actions depend on previous actions) that are needed to be performed to complete the task as well as any words to output. These policies are trained to match the demonstrator\u0026rsquo;s policies via Behavior Cloning. Figure 13: Model Architecture for using BC to Create Policies from Image and Language Data Using Inverse Reinforcement Learning (IRL) to create Reward Model Note that the Reward model similarly receives its inputs to the Policy model above: by using image and text data, converting these modalities of data into embeddings, fusing these embeddings, and using a multimodal transformer to understand the fused embeddings. The discriminator loss is used similarly as GAIL: to determine whether the trajectories were produced by the agent or if they were sampled from the demonstrations produced by experts. As GAIL is difficult to use with high-dimensional data, the authors also use multiple strategies outlined below to make GAIL effective for the setup chosen such as random image augmentation (ex: cropping, rotating, translating images), language matching, and object-in-view. The authors employed language matching as an auxiliary task for representation learning that checks whether the instruction created by the setter and the images inputted come from the same episode or not. This would require the ResNet to produce embeddings that have a high degree of mutual information with the tokenized inputs. The authors employed object-in-view as another auxiliary task for representation learning that asks the agent to predict whether a certain object is in the image data provided to the agent or not. This requires the ResNet to produce embeddings that reflect the objects present in the image data. Figure 14: Model Architecture for using IRL to Create Reward Models from Image and Language Data Results In study A below, the authors found that using behavior cloning and GAIL was more effective than simply learning behavior cloning and not learning the reward model for tasks that required following instructions. However, using GAIL to learn a reward model did not seem to significantly boost performance for question-answering tasks. This is because GAIL can help the agent understand what a task truly means, which is helpful for instruction following, while behavior cloning can not. On the other hand, behavior cloning can simply mimic the mapping from questions to answers that the demonstrator uses so it tends to be as effective as using behavior cloning and GAIL for Q\u0026amp;A tasks. In study B below, the authors also found that for a given task, such as finding the count of a certain object in a room, using data for other tasks for training, such as finding the position/color of an object, led to better performance over limiting the training data to only include demonstrations that directly corresponded to the task that was to be performed. This is because more diverse data and seemingly unrelated commands allow tasks to inform each other (due to some crossover), allows the agent to better understand the language of the commands it receives, and how this language is grounded in the environment. Study C shows that this phenomenon also helps with data efficiency; when trained with all types of prompts, the model only needed 1/8th as much data as training with only the \u0026ldquo;Position\u0026rdquo; prompt to reach the same level of performance. Study D similarly shows object-color generalization capabilities as the model was able to sufficiently perform tasks dealing with orange ducks such as \u0026ldquo;lift an orange duck\u0026rdquo; even when all instances of orange ducks were removed from the dataset. Figure 15: Graphs for some of the studies performed by the authors (described in detail above) Advantages \u0026amp; Disadvantages Advantages Generalization: Language commands learned are general and compositional Large Multi-Task Model: Agents do not need to learn sub-policies. They can simply learn a set of behaviors that can be used for a variety of tasks These behaviors also inform each other, leading to better performance for any task when compared to using a single-task approach Production of Language: Agents are not only able to understand the language commands but are also able to produce them Such an ability may allow the agent to explain itself Disadvantages Data Collection and Complex Training Strategies: This paper required 2+ years of data collection in a complex simulator that required 2 demonstrators interacting. Due to the expensiveness of the data collection, this paper is rather difficult to reproduce. Also, the complex tricks required may mean that the results are limited to the environment that the authors have set up and the constrained vocabulary (~550 unique words) that they are using Independent Training: Agent cannot actively ask for help on new problems. For example, the agent cannot ask to see more data on tasks that it has not been asked to perform before or on objects that it has not seen before Cannot Quickly Acquire New Knowledge: Since many data points are required and a high amount of compute is necessary for training, it is difficult for the model to learn new tasks or to gain new insights quickly, as this would require more data points for those tasks and the model would have to be re-trained from scratch. In other words, it is \u0026ldquo;set\u0026rdquo;. Future Research Directions Methods to learn from language that is representative of experiences in the \u0026ldquo;real world\u0026rdquo;, rather than using language that is crawled from the Internet Ground language in task execution and objects in the environment and use language is taken directly from humans, rather than just using tweets from Twitter or posts on Reddit Learning to use language for Active Learning Creating capabilities for the agent to ask for assistance, clarification, or to remove ambiguities using language, rather than just using offline learning that often requires many data points Creating efficient systems of NLP + RL Creating a model that can efficiently learn new tasks and knowledge beyond its existing knowledge base in an efficient manner Also, agents must learn to think and act quickly to be used in real-world situations where language instructions are given constantly Learning interactively Creating capabilities for the agent to ask for more demonstrations on a certain task or other forms of assistance so that the agent is more involved in the learning process (more like \u0026ldquo;situated learning\u0026rdquo;) Using Audio Data Audio data has previously not been used due to its high dimensionality. However, acoustics can influence the meaning of commands; for example, giving a command in a sarcastic tone is quite different from giving it in an urgent tone Also, the volume and duration of the acoustics could indicate whether the agent is receiving the command or whether the command was intended for someone else (allows the agent to learn from noisy commands, which typically occurs in the real world) Key Takeaways Using language can help solve agents solve many tasks but it remains difficult due to the problem of grounding the input. There are several representations we can use for language such as sequences of tokens, dictionary indices, Word2Vec, GloVe, RNNs, LSTMs, and Transformers, with each representation having its advantages and disadvantages. To unify task-learning and language for LfD, there are three main approaches we discussed: structure language approaches, low-level language-task alignment, and high-level language-task alignment (note that each approach is more complex yet more applicable in the real world than the previous). Leveraging natural language is still an open area of research with current approaches trying to leverage audio or trying to make these approaches more interactive, efficient, and applicable in the real world. ","date":"2022-06-01T00:00:00Z","permalink":"http://deidnani.github.io/p/leveraging-natural-language/","title":"Leveraging Natural Language"}]