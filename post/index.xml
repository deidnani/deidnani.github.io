<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Beyond the Buzz</title>
        <link>http://deidnani.github.io/post/</link>
        <description>Recent content in Posts on Beyond the Buzz</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 11 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://deidnani.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Video-and-Language Pre-Training with AlPRO</title>
        <link>http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/</link>
        <pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/</guid>
        <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;A recent trend in AI is to solve tasks that use multiple modalities (text, audio, images, video, etc.). As part of this effort, one specific area is video-and-language understanding, which includes tasks such as text-video retrieval, video question answering, and video captioning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks.png&#34;
	width=&#34;660&#34;
	height=&#34;478&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks_hube1e6d1256efda1602d77307a85c0770_21189_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/video-language-tasks_hube1e6d1256efda1602d77307a85c0770_21189_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;331px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: Examples of video-language tasks from the VALUE (Video-and-Language Understanding Evaluation) paper&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since there are so many tasks in this field and new tasks will likely appear in the near future, researchers have worked to create unified models that can solve many of these tasks. One such approach to create a unified model is to develop a &lt;strong&gt;pretraining&lt;/strong&gt; strategy that develops meaningful representations for the video and text which can then be used for any of these tasks.&lt;/p&gt;
&lt;h2 id=&#34;difficulties-with-video-language-pre-training&#34;&gt;Difficulties with Video-Language Pre-Training&lt;/h2&gt;
&lt;p&gt;However, there are several difficulties with creating such a model, some of which are outlined below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compared to images, consecutive frames in videos may convey the same information or may be slightly different and we would be wasting memory and computation power if we use every single frame to solve a specific task (especially in an online system).&lt;/li&gt;
&lt;li&gt;The feature representations of videos and text are often in completely different vector spaces and understanding the interactions between these two is not as trivial, often leading to &lt;strong&gt;misalignment&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Tasks used to train these pre-trained models typically focus on the high-level information conveyed in a video rather than specific, &lt;strong&gt;fine-grained&lt;/strong&gt; information such as the objects shown.&lt;/li&gt;
&lt;li&gt;Videos also have the additional difficulty unlike images of conveying &lt;strong&gt;temporal&lt;/strong&gt; information: the easy solution of simply using the frames of the video independently of each other completely ignores this information. In other words, the model doesn&amp;rsquo;t understand that the video is a set of frames that take place after each other.&lt;/li&gt;
&lt;li&gt;Since annotation is expensive and sometimes subjective (especially in the case of video captioning), there is a lack of large datasets of data that are manually annotated, so approaches often need to use unlabeled data or create &lt;strong&gt;pseudo-labels&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this literature review, we will be looking at a papers that focus on solving these challenges called &lt;strong&gt;ALPRO&lt;/strong&gt; by Salesforce Research, which focuses on solving the misalignment problem through a &lt;em&gt;video-text contrastive&lt;/em&gt; (VTC) loss and &amp;ldquo;lack of fine-grained information&amp;rdquo; problem through a &lt;em&gt;prompting entity modeling&lt;/em&gt; (PEM) pretraining task.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;AlPRO first focuses on creating representations for the text and video independently (called &lt;strong&gt;unimodal encoding&lt;/strong&gt;) and then combining these representations with a multimodal encoder to capture the interactions between the two. To make sure that the representations of the video and text align well before being sent to the encoder, the model uses a video-text contrastive loss, which pushes the representations of the similar texts and video to be closer to each other and vice-versa. To capture fine-grained visual information, AlPRO uses a prompting entity modeling (PEM) pretraining objective, where the model is asked to predict the objects/nouns (called &lt;strong&gt;entities&lt;/strong&gt;) from random crops that are extracted from the video. Since the labels for these entities are not available, a &lt;em&gt;entity prompter&lt;/em&gt; module is added to create &amp;ldquo;pseudo-labels&amp;rdquo; as proxies for the true labels. These pseudo-labels are essentially similarity scores that it calculates between the random crops and a set of sentences (called &lt;strong&gt;prompts&lt;/strong&gt;) of the form &amp;ldquo;A video of {entity}&amp;rdquo;. These entities in its &amp;ldquo;dictionary&amp;rdquo; are sampled from nouns that were found in the pretraining data (called &lt;strong&gt;corpus&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example.png&#34;
	width=&#34;553&#34;
	height=&#34;595&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example_hu33cbd42a934ca63b69fe3e78b9e3124e_197182_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/alpro-example_hu33cbd42a934ca63b69fe3e78b9e3124e_197182_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;223px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: An example of how AlPRO differs from previous approaches and how it uses the entity prompter module for its pretraining objective&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;deep-dive&#34;&gt;Deep Dive&lt;/h2&gt;
&lt;h3 id=&#34;encoding-the-video-and-text&#34;&gt;Encoding the Video and Text&lt;/h3&gt;
&lt;h4 id=&#34;visual-encoder&#34;&gt;Visual Encoder&lt;/h4&gt;
&lt;p&gt;For the visual encoder, a 12-layer &lt;strong&gt;TimeSFormer&lt;/strong&gt; is used which takes input frames of size $224 \times 224$. This model works in the following way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each frame is split into $K$ patches which are then encoded to produce representations for each patch. For example, in the image below we can see that $K = 16$.&lt;/li&gt;
&lt;li&gt;Similar to standard transformers, &lt;strong&gt;positional embeddings&lt;/strong&gt; are also created so that the model is aware of the position of different patches in the image (since the position and content of the patch both convey important information). For example, the model needs to understand that the patch showing &amp;ldquo;fingers&amp;rdquo; is close to the patch showing &amp;ldquo;hand&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Temporal and spatial attention is then performed to create $K \times d$ features for each frame. The way this is done is shown below. Think of &lt;strong&gt;attention&lt;/strong&gt; as an &amp;ldquo;information sharing&amp;rdquo; mechanism between patches in the same frame or across frames where patches choose which other patches they want to receive information from. &lt;strong&gt;Space attention&lt;/strong&gt; means that information can only be shared between patches in the same frame and not across frames. &lt;strong&gt;Joint space-time attention&lt;/strong&gt; means that information can shared between every patch in every frame. However, to improve efficiency, &lt;strong&gt;Divided space-time attention&lt;/strong&gt;, which the TimeSformer uses, means that information can be shared between a particular patch and every patch in the same frame (just like space attention) but also the same patch in all other frames.&lt;/li&gt;
&lt;li&gt;The features are then averaged across frames to create a single $K \times d$ vector for the whole video. Other than this $K \times d$ vector, similar to other transformers, a representation for the [CLS] token is added which can receive information from all the patches in the video across all frames and thus can be seen as a single $d$-dimensional representation for the entire video.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention.png&#34;
	width=&#34;600&#34;
	height=&#34;531&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention_huf6edaf166425396bd3767ea35ccb2b49_319177_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/timesformer-attention_huf6edaf166425396bd3767ea35ccb2b49_319177_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;271px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Example of the divided space-time attention that is used in the TimeSformer compared to standard space attention and attending across the entire space across time (joint space-time attention)&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;text-encoder&#34;&gt;Text Encoder&lt;/h4&gt;
&lt;p&gt;For text, a standard 6-layer transformer is used to create representations for the text from the first 6 layers of a pretrained text transformer known as $BERT_{base}$. The text is split into tokens and each token is encoded to produce a $d$-dimensional representation. Similar to the visual encoder, positional embeddings are added and the [CLS] token is used to contain information of the entire text sequence as it can receive information from all tokens in the text sequence.&lt;/p&gt;
&lt;h4 id=&#34;multimodal-encoder&#34;&gt;Multimodal Encoder&lt;/h4&gt;
&lt;p&gt;The multimodal encoder is a standard transformer that takes the representations of the video and text and aggregates them to create a combined representation. The video and text representations are directly concatenated to produce a $(N_v + N_t) \times d$ vector (where $N_v \times d$ was the size of the video representation and $N_t \times d$ was the size of the text representation) outputted by the multimodal encoder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders.png&#34;
	width=&#34;607&#34;
	height=&#34;318&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders_hu7aa39442e4f59237e884156c0d7eff22_53893_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/encoders_hu7aa39442e4f59237e884156c0d7eff22_53893_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;458px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 4: An outline of how the video and text are encoded and fed into a multimodal video-text encoder&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;pretraining&#34;&gt;Pretraining&lt;/h3&gt;
&lt;p&gt;ALPRO is trained on four objectives, including two traditional ones such as &lt;strong&gt;masked language modeling&lt;/strong&gt; (MLM) and &lt;strong&gt;video-text matching&lt;/strong&gt; (VTM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In MLM, the model is asked to predict text tokens that are masked out; specifically, certain tokens in a sentence are replaced with [MASK] and the model is asked to predict what the original token was.&lt;/li&gt;
&lt;li&gt;In VTM, the model is asked whether a video and text description are matching or not. Here, the [CLS] token of the multimodal representation is used (as it is a $d$-dimensional vector that represents both the video and text). To get video-text pairs that aren&amp;rsquo;t matching (also known as &lt;strong&gt;negative samples&lt;/strong&gt;), random videos and pieces of text are sampled from the training data. Specifically, &lt;strong&gt;hard negatives&lt;/strong&gt; are sampled through several ways so that the model finds it challenging to distinguish positive video-text pairs from negative ones, rather than just randomly sampling videos and pieces of text where it is easy for the model to figure out that the video and text are not matching.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will now focus on the novel objectives introduced: the video-text contrastive (VTC) loss and prompting entity-modeling (PEM) losses.&lt;/p&gt;
&lt;h4 id=&#34;video-text-contrastive-loss-vtc&#34;&gt;Video-Text Contrastive Loss (VTC)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, [CLS] embeddings for both the video and text are projected to a lower-dimensional ($256$-dimensional) space rather than the high, $d$-dimensional representation that was used previously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, similarity scores are then calculated by taking the dot-product of both embeddings (Dot products typically measure how similar two vectors are).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The video-text contrastive loss consists of two terms, both of which use these similarity scores.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Firstly, a single video $V_i$ is compared to several pieces of text $T$ and we want to encourage the representation for $V_i$ to be similar to its most similar instance of text in $T$ which we will denote as $T_i$. Below, we can see that we&amp;rsquo;re comparing the similarity score of $(V_i, T_i)$ to the similarity scores for all other text $T_j$ in the batch through what is known as the &lt;strong&gt;softmax&lt;/strong&gt; function and then the logarithm is applied. $\tau$ is often known as the &lt;strong&gt;temperature&lt;/strong&gt; and scales the objective to better learn from the hard negatives.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{v2t} = -\log \frac{exp(s(V_i, T_i)/\tau)}{\sum_j exp(s(V_i, T_j)/\tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 1: First Component of Video-Text Contrastive Loss where a single video is compared to several pieces of text&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Similarly, a single piece of text $T_i$ is compared to several pieces of video $V$ and we want to encourage the representation for $T_i$ to be similar to its most similar instance of video in $V$ which we will denote as $V_i$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{t2v} = -\log \frac{exp(s(T_i, V_i)/\tau)}{\sum_j exp(s(T_i, V_j) / \tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 2: Second Component of Video-Text Contrastive Loss where a single text is compared to several pieces of video&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;prompting-entity-modeling-pem-loss&#34;&gt;Prompting Entity-Modeling (PEM) Loss&lt;/h4&gt;
&lt;p&gt;Firstly, note that the entity prompter has a similar architecture to the video-language pre-training model without the multimodal video-text encoder as shown below. In other words, the visual and text encoders and the VTC loss are also used to create the entity prompter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter.png&#34;
	width=&#34;451&#34;
	height=&#34;177&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter_hu52a093c8ffd748f120964b726a419751_38581_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/entity-prompter_hu52a093c8ffd748f120964b726a419751_38581_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 5: An outline of how the video and text are encoded for the entity-prompter&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The entity prompter maintains a list of $M$ text entity prompts as explained above of the form &amp;ldquo;A video of {Entity}&amp;rdquo;. The entities are gathered by using a POS (part-of-speech) tagger and using the top 1,000 most frequent entities in the dataset.&lt;/li&gt;
&lt;li&gt;For each of these prompts, the [CLS] token is pre-computed to be used later.&lt;/li&gt;
&lt;li&gt;To generate the pseudo labels, we create random video crops $\hat{V}$ of the video and the corresponding [CLS] embedding using the visual encoder. Note that only crops that occupy 30-50% of the original spatial area of the frame are used.&lt;/li&gt;
&lt;li&gt;Next, we create the pseudo-label by finding the normalized similarity between the the crop [CLS] token $\hat{v_{cls}}$ and the prompt [CLS] tokens $t_{cls}$, with a formula similar to the contrastive-losses presented above. In other words, we are measuring the similarity between the random crop and the prompts in our &amp;ldquo;dictionary&amp;rdquo;. Note that only pseudo-labels that are above a certain threshold (0.2) are included in the calculations; in other words, we only include prompts that are &amp;ldquo;similar enough&amp;rdquo; to the crop that we have taken.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ q_{\hat{V}, m} = \frac{exp(s(\hat{V}, T_m) / \tau)}{\sum_m exp(s(\hat{V}, T_m) / \tau)} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 3: Formula for the pseudo-labels for the entity prompter&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;During pre-training, we take the embeddings from the multimodal encoder corresponding to the specific video crop and average them to create a single $d$-dimensional vector and then feed them to a classifier to compute the entity prediction $p_{\hat{V}}$. This is done similarly as above except we don&amp;rsquo;t use the similarity scores calculated by the entity prompter but the embeddings directly produced by the multimodal encoder.&lt;/li&gt;
&lt;li&gt;Next, a &lt;strong&gt;cross-entropy&lt;/strong&gt; loss between $p_{\hat{V}}$ and $q_{\hat{V}}$ is used as our final loss. Note that the cross-entropy loss encourages the two distributions (and accordingly, vectors) $p$ and $q$ to be as similar as possible as the term grows larger when there&amp;rsquo;s an inconsistency between $p$ and $q$ (we predict a different label than the pseudo-label) and vice-versa. In other words, we want our video-language pre-training model to find the same prompts similar to a particular video crop as the entity prompter does, and this is accomplished through the loss function shown below.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ L_{PEM} = -\sum_m q_{\hat{V}, m} \log p_{\hat{V}, m} $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Equation 4: PEM loss which is simply cross-entropy between the pseudo-labels and the video-text model&amp;rsquo;s predictions for the entities&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework.png&#34;
	width=&#34;1099&#34;
	height=&#34;439&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework_huea0b605ba29f72fa144a633967b775b4_112952_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/full-framework_huea0b605ba29f72fa144a633967b775b4_112952_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;600px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 6: An outline of the full framework&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;pre-training-data&#34;&gt;Pre-training Data&lt;/h3&gt;
&lt;p&gt;For pre-training data, the authors use WebVid-2M, which contains 2.5 million video-text pairs with static videos created by arranging duplicates from images in the CC-3M dataset into videos.&lt;/p&gt;
&lt;h3 id=&#34;experiment-data&#34;&gt;Experiment Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For text-video retrieval, the MSRVTT dataset is used which contains 10,000 videos with 200,000 text captions.&lt;/li&gt;
&lt;li&gt;For video-question answering, the MSVD-QA dataset is uised which contains 1,970 videos and 50,000 question answer pairs with 2,423 answer candidates and MSVRTT-QA which uses videos/captions from MSRVTT with 243,000 questions and 1,5000 answer candidates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The authors find that both losses improve performance across all tasks and pseudo-labels are diverse and meaningful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VTC is found to be important for the text-video retrieval task since it solves the alignment problem between videos and text.&lt;/li&gt;
&lt;li&gt;PEM helps for question-answering due to its ability to learn specific features in videos. However, performance is worse on MSVD-QA since MSVD-QA focuses on more region-level knowledge and only PEM plays a key role while MSRVTT-QA focuses on coarser-grained information so both losses play a key role.&lt;/li&gt;
&lt;li&gt;When compared to other approaches, the authors find that AlPRO surpasses other methods substantially with a &lt;strong&gt;6%&lt;/strong&gt; improvement in performance with video-text retrieval and on-par results for video question answering even though much &lt;strong&gt;less data&lt;/strong&gt; is used compared to other methods such as VQA-T which use 69 million QA pairs for training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels.png&#34;
	width=&#34;730&#34;
	height=&#34;418&#34;
	srcset=&#34;http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels_hua53b5db7787f0b95ee399cba88962330_337694_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/video-and-language-pre-training-with-alpro/images/pseudo-labels_hua53b5db7787f0b95ee399cba88962330_337694_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 7: Examples of the pseudo-labels produced for an image&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Through ablations, authors find using multiple prompts (or templates) other than  just the single template of &amp;ldquo;A video of a {Entity}&amp;rdquo; such as &amp;ldquo;A footage of one {Entity}&amp;rdquo; leads to additional gains in performance.&lt;/li&gt;
&lt;li&gt;Additionally, keeping the number of entities low enough so that only the most frequent entities are added to the &amp;ldquo;dictionary&amp;rdquo; seems to play an important role as well.&lt;/li&gt;
&lt;li&gt;Keeping the number of frames low is also important as the authors find that increasing the number of frames up to 8 increases performance but limited gains are seen when the number of frames is increased beyond 8.&lt;/li&gt;
&lt;li&gt;Through these ablations and other analyses, the authors plan to focus more on creating better prompts as well as using temporal information to select the crops in the videos in the near future.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Leveraging Natural Language</title>
        <link>http://deidnani.github.io/p/leveraging-natural-language/</link>
        <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
        
        <guid>http://deidnani.github.io/p/leveraging-natural-language/</guid>
        <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;h3 id=&#34;why-do-we-want-to-use-language&#34;&gt;Why do we want to use language?&lt;/h3&gt;
&lt;p&gt;Integrating Natural Language into robotic tasks can provide added convenience and interactivity for humans. For example, it can enable robotic tasks such as language-based navigation and object retrieval in which the robot is given prompts like &amp;ldquo;Get me a clean cup from the kitchen&amp;rdquo; and the robot must act accordingly.&lt;br&gt;
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/introduction.png&#34;
	width=&#34;1147&#34;
	height=&#34;529&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/introduction_hu599246d70d9369bbd751745c0a227530_409202_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/introduction_hu599246d70d9369bbd751745c0a227530_409202_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: An example of how a robot using language and current perception to produce an action ($a_t$)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, language enables us to provide both high-level and low-level guidance. For example, rather than just telling the robot &amp;ldquo;Get me a clean cup from the kitchen&amp;rdquo; we can also tell it to &amp;ldquo;Go straight, then take a right, then walk straight to the kitchen counter and pick up the cup.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Additionally, language is information-rich since it adds flexibility. The user can decide how much information it wants to give and the agent must infer using the provided information along with the data it has already seen so far.&lt;/p&gt;
&lt;p&gt;Finally, several tasks require language inputs. In addition to the language-based navigation application shown above, other tasks include translation, chat-based guidance, and question answering. 
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/questionAnswering.jpg&#34;
	width=&#34;2667&#34;
	height=&#34;1500&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/questionAnswering_hudc344a8671a4cb0773cc35ed6a3d9861_426049_480x0_resize_q75_box.jpg 480w, http://deidnani.github.io/p/leveraging-natural-language/images/questionAnswering_hudc344a8671a4cb0773cc35ed6a3d9861_426049_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;
&lt;em&gt;Figure 2: An example of visual question answering, which combines computer vision with natural language processing.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To apply natural language to various tasks in computer vision and robotics, we must first ground the input. &lt;strong&gt;Grounding&lt;/strong&gt; refers to mapping language-based inputs to robot or agent behavior. Grounding language to objects in the domain, actions the robot must take, and agent rewards are common examples of tasks revolving around this concept. However, grounding is an inherently hard problem since it involves understanding the underlying meaning of the language input. Since grounding is central to the success of language-based robots, it is often challenging to achieve high performance on language-based tasks.
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/grounding.jpg&#34;
	width=&#34;525&#34;
	height=&#34;640&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/grounding_hu2b2ef60d909db22dc81767e5071a39e0_92099_480x0_resize_q75_box.jpg 480w, http://deidnani.github.io/p/leveraging-natural-language/images/grounding_hu2b2ef60d909db22dc81767e5071a39e0_92099_1024x0_resize_q75_box.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;82&#34;
		data-flex-basis=&#34;196px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: An example of how grounding can be used to convert action phrases to executable actions.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;representation&#34;&gt;Representation&lt;/h2&gt;
&lt;p&gt;To train our models on language, we must first convert the language input into a representation the computer can understand. The first step in this process is breaking the sentence so that the model can learn about the correlation between sentences in our dataset.&lt;/p&gt;
&lt;h3 id=&#34;sequence-of-tokens&#34;&gt;Sequence of Tokens&lt;/h3&gt;
&lt;p&gt;We often use a representation called &lt;strong&gt;sequence of tokens&lt;/strong&gt; to represent each sentence where &lt;strong&gt;token&lt;/strong&gt; refers to each element in the &lt;strong&gt;vocabulary&lt;/strong&gt; used by the model. Hence, the size of the vocabulary is depended upon the tokenization we choose to represent our inputs with. Some common choices include tokenization by individual words, by characters, and by Byte-Pair Encodings (BPE). For the sentence &lt;code&gt;Fetch me the vacuum cleaner&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;Fetch&amp;gt;&amp;lt; &amp;gt;&amp;lt;me&amp;gt;&amp;lt; &amp;gt;&amp;lt;the&amp;gt;&amp;lt; &amp;gt;&amp;lt;vacuum&amp;gt;&amp;lt; &amp;gt;&amp;lt;cleaner&amp;gt;&lt;/code&gt; corresponds to the tokenization by individual words.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;F&amp;gt;&amp;lt;e&amp;gt;&amp;lt;t&amp;gt;&amp;lt;c&amp;gt;&amp;lt;h&amp;gt;&amp;lt; &amp;gt;&amp;lt;m&amp;gt;&amp;lt;e&amp;gt;&amp;lt; &amp;gt;&amp;lt;t&amp;gt;&amp;lt;h&amp;gt;&amp;lt;e&amp;gt;&amp;lt; &amp;gt;&amp;lt;v&amp;gt;&amp;lt;a&amp;gt;&amp;lt;c&amp;gt;&amp;lt;u&amp;gt;&amp;lt;u&amp;gt;&amp;lt;m&amp;gt;&amp;lt; &amp;gt;&amp;lt;c&amp;gt;&amp;lt;l&amp;gt;&amp;lt;e&amp;gt;&amp;lt;a&amp;gt;&amp;lt;e&amp;gt;&amp;lt;r&amp;gt;&lt;/code&gt; corresponds to the tokenization by individual characters.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;Fet&amp;gt;&amp;lt;ch&amp;gt;&amp;lt; &amp;gt;&amp;lt;me&amp;gt;&amp;lt; &amp;gt;&amp;lt;the&amp;gt;&amp;lt; &amp;gt;&amp;lt;va&amp;gt;&amp;lt;cu&amp;gt;&amp;lt;um&amp;gt;&amp;lt; &amp;gt;&amp;lt;clean&amp;gt;&amp;lt;er&amp;gt;&lt;/code&gt; correponds to the tokenization by Byte-Pair encodings.&lt;/p&gt;
&lt;p&gt;Byte-Pair Encodings have been discovered to be the perfect middle ground between tokenization by characters and by words. The biggest advantage of using BPE is the breakdown of rare words into common tokens so that the model can still understand the meaning of the word by combing those tokens. The tokens used for BPE can either be derived from a predefined vocabulary such as the ones used by popular models today or can be created using the following algorithm:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1. Initialize vocabulary with individual characters and sentences with character tokenization. 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2. Iteratively count the frequency of each token pair in the current sentence tokenization.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;3. Merge every occurrence of the most frequent pair and the merged pair to the vocabulary.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;4. Repeat steps 2 and 3 till the desired vocabulary size is achieved.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The next step in representing sentences is converting each token into an array of integers and/or floats which can be input into our model. The four common strategies for achieving this representation are dictionary indices, Word2Vec/GloVe, RNN/LSTM, and Transformers.&lt;/p&gt;
&lt;h3 id=&#34;dictionary-indices&#34;&gt;Dictionary Indices&lt;/h3&gt;
&lt;p&gt;Using dictionary indices is the easiest way of converting tokens into an array of floats. First, we order the words in our dictionary from &lt;code&gt;0 to the size of the dictionary - 1&lt;/code&gt;. Then, we use a one-hot vector of &lt;code&gt;length = size of dictionary&lt;/code&gt; to represent each token, where all elements are $0$ except the value at the token&amp;rsquo;s index in the dictionary, which contains a $1$. For example, if a dictionary contains 2000 elements and if the token &lt;code&gt;Machine&lt;/code&gt; is at the 342nd index in the dictionary, then its encoding will be an array of length 2000 containing 0s at all places except 1 at the 342nd index.&lt;/p&gt;
&lt;h3 id=&#34;word2vec-and-glove&#34;&gt;Word2Vec and GloVe&lt;/h3&gt;
&lt;p&gt;A one-hot vector for each token is impractical because our dictionary can have thousands of tokens. Word2Vec and GloVe are two common ways of converting one-hot vector representation of tokens to a smaller dimensional vector. 
&lt;strong&gt;Word2Vec&lt;/strong&gt; refers to a group of shallow $2$-layer neural nets that can be used to create these smaller dimensional vectors. The neural nets can either use a continuous bag-of-words architecture or a continuous skip-gram architecture. Both models have a hidden layer that can be used after training to get word embedding. The continuous bag-of-words architecture refers to a feed-forward model that uses surrounding words to predict the next word. The skip-gram architecture on the other hand uses the current word to predict surrounding words.
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/skipGram.png&#34;
	width=&#34;815&#34;
	height=&#34;454&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/skipGram_hu169f1368e1c8a388003d7fcbaa0abc8e_13039_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/skipGram_hu169f1368e1c8a388003d7fcbaa0abc8e_13039_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;179&#34;
		data-flex-basis=&#34;430px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 4: Bag-of-Words and Skip-Gram architectures used for Word2Vec&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GloVe&lt;/strong&gt; also known as Global Vectors is an unsupervised algorithm for obtaining smaller dimensional vector representations. Global word-word co-occurrence statistics are first extracted from a training corpus and then a log-bilinear regression model is trained. The regression model combines global matrix factorization and local context window methods.
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/glove.png&#34;
	width=&#34;958&#34;
	height=&#34;528&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/glove_hu4e44d0ae1fc07ca43fbf1ad2dc09c40d_41775_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/glove_hu4e44d0ae1fc07ca43fbf1ad2dc09c40d_41775_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;435px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 5: Comparison of GloVe embeddings on different words. Values are normalized between $1$ (blue) and $-1$ (red).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One of the most important qualities of both embeddings is that they preserve similarity between different sentences that have the same meaning.&lt;/p&gt;
&lt;h3 id=&#34;rnnlstm&#34;&gt;RNN/LSTM&lt;/h3&gt;
&lt;p&gt;Long-Short Term Memory (LSTM) models, which are a special type of Recurrent Neural Networks, can also be used to create vector representation of tokens. Additionally, they can also be used to create a vector representation of the whole sentence. 
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/lstm.png&#34;
	width=&#34;2014&#34;
	height=&#34;1322&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/lstm_hu6bee907bb3498980c7db25301095e2a4_191690_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/lstm_hu6bee907bb3498980c7db25301095e2a4_191690_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 6: A diagram of the internal structure of an LSTM cell. The model typically consists of several such cells with either weight shared. During training, the outputs from the current cell become the input into the next cell.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The hidden vector $h_t$ corresponds to the vector representation of each token while the last context vector $c_t$ corresponds to the vector representation of the whole sentence after training. LSTMs for vector representations are typically trained in an end-to-end fashion alongside the primary task.&lt;/p&gt;
&lt;h3 id=&#34;transformer&#34;&gt;Transformer&lt;/h3&gt;
&lt;p&gt;The Transformer is the current state-of-the-art model not only for encoding sentences but also grounding language in general. Transformers are particularly powerful because of multi-head self-attention which allows them to weigh different subsets of the input simultaneously.
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/transformer.png&#34;
	width=&#34;1999&#34;
	height=&#34;1151&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/transformer_hufe8d0a8591c0916acc1939d299069572_376109_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/transformer_hufe8d0a8591c0916acc1939d299069572_376109_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;173&#34;
		data-flex-basis=&#34;416px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 7: Model Architecture of the Transformer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, Transformers are being applied to a variety of tasks, from NLP-oriented translation and text summarization tasks to multimodal objectives such as visual question answering and navigation. Transformers can be easily modified to take in multimodal inputs by modifying the input and output embedding layers to process images using CNNs. Image embeddings can then be concatenated with token embeddings to get a frame-by-frame multimodal embedding of the task.&lt;/p&gt;
&lt;h2 id=&#34;unifying-task-language-data&#34;&gt;Unifying task-language data&lt;/h2&gt;
&lt;p&gt;One example of how language data can help in executing a certain task was outlined in the Action2Vec framework that combined language data with videos.
The framework did the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create embeddings from video frames using a CNN&lt;/li&gt;
&lt;li&gt;Embed features extracted from CNN into a single vector using an LSTM&lt;/li&gt;
&lt;li&gt;Similarly, for words, create embeddings from words using Word2Vec and embed features extracted into a single vector using an LSTM or a pooling operation (avg/max pooling)&lt;/li&gt;
&lt;li&gt;Use a Pairwise Ranking Loss that takes these dense vectors and makes sure that words and videos that share the same concepts have similar embeddings and different words and videos have different embeddings&lt;/li&gt;
&lt;li&gt;Once, these embeddings are created, we can perform &amp;ldquo;arithmetic&amp;rdquo; on these embeddings. For example below, we take the embeddings for frames of a person playing piano and we try to create a video that shows a person playing a &amp;ldquo;violin&amp;rdquo; instead of a &amp;ldquo;piano&amp;rdquo;. This can be done by getting the mean Action2Vec embeddings for videos corresponding to piano playing, subtracting the Word2Vec embedding for &amp;ldquo;piano&amp;rdquo;, adding the Word2Vec embedding for &amp;ldquo;violin&amp;rdquo;, and searching the Action2Vec embedding space for all frames that are similar to the resulting vector. 
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/Action2Vec.png&#34;
	width=&#34;853&#34;
	height=&#34;309&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/Action2Vec_hu758e0a971cc05d02657e5eaf25e80874_116816_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/Action2Vec_hu758e0a971cc05d02657e5eaf25e80874_116816_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;276&#34;
		data-flex-basis=&#34;662px&#34;
	
&gt;
&lt;em&gt;Figure 8: Model Architecture of the Action2Vec Framework&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;structured-language-approaches&#34;&gt;Structured language approaches&lt;/h2&gt;
&lt;h3 id=&#34;what-is-structured-language&#34;&gt;What is structured language?&lt;/h3&gt;
&lt;p&gt;Structured language involves directly mapping words to concepts that the robot already knows. This is done by parsing the word sequence and tagging each word with its semantic purpose. These parsed sequences are mapped to pre-made logical primitives that the robot can interpret. One way these logical primitives can be represented is through linear temporal logic (LTL) specification. Also, this structured language approach requires a decent amount of overhead since the robot already needs to completely know all the actions available to it as well as the direct mapping from words to those actions.&lt;/p&gt;
&lt;p&gt;As shown in figure 9, an example approach involves:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Tagging and parsing - Assigning a hierarchical structure to a sentence&lt;/li&gt;
&lt;li&gt;Null element restoration - Allows the structure of imperatives to be directly matched by a semantic interpretation module&lt;/li&gt;
&lt;li&gt;VerbNet frame matching - Identifies verbs and their arguments in parse trees using VerbNet&lt;/li&gt;
&lt;li&gt;LTL Formula Generation - Maps to existing logical primitive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/ltl.png&#34;
	width=&#34;1247&#34;
	height=&#34;246&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/ltl_hue9626b3da46b4f9b03fb952da34abc76_98760_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/ltl_hue9626b3da46b4f9b03fb952da34abc76_98760_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;506&#34;
		data-flex-basis=&#34;1216px&#34;
	
&gt;
&lt;em&gt;Figure 9: Conversion of the sentence &amp;ldquo;Go to the hallway&amp;rdquo; into LTL formulas through tagging, parsing, null element restoration, semantic interpretation, and LTL formula generation&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Although expensive to develop, this structure allows humans to directly inspect generated plans. This means that the system has guarantees and is verifiable. It is also simple to diagnose failures since researchers can investigate contradictions in planned behaviors.&lt;/p&gt;
&lt;h3 id=&#34;advantages--disadvantages&#34;&gt;Advantages &amp;amp; Disadvantages&lt;/h3&gt;
&lt;h4 id=&#34;advantages&#34;&gt;Advantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generalization:
&lt;ul&gt;
&lt;li&gt;It is possible to ensure that commands map directly to task executions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Interpretability:
&lt;ul&gt;
&lt;li&gt;Researches can directly analyze task executions to the language since it is human-readable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Failure analysis:
&lt;ul&gt;
&lt;li&gt;It is easy to debug since one can directly trace out the plans and language commands to resolve issues&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;disadvantages&#34;&gt;Disadvantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Brittle:
&lt;ul&gt;
&lt;li&gt;The wording for the actions has to be very exact meaning the researcher need to memorize the wording for all the actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Expertise Required:
&lt;ul&gt;
&lt;li&gt;Non-experts may have difficulties creating logic primitives and connecting them to corresponding sentence parses&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ambiguity:
&lt;ul&gt;
&lt;li&gt;The speaker needs to be exact in wording and not give under-specified commands (ex. “leave” instead of “leave the room”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;low-level-language-task-alignment&#34;&gt;Low-level language-task alignment&lt;/h2&gt;
&lt;h3 id=&#34;policy-sketches&#34;&gt;Policy Sketches&lt;/h3&gt;
&lt;p&gt;Policy sketches involve having low-level planners for each specific task and then having a high-level model that activates these sub-components to complete the full action. This includes training an overarching network that can understand a full task specification and then will use existing sub-policies when needed. 
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/policySketches.png&#34;
	width=&#34;1181&#34;
	height=&#34;359&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/policySketches_huac224bd8b6b28dc3e3a0c1639a4726ef_347377_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/policySketches_huac224bd8b6b28dc3e3a0c1639a4726ef_347377_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;328&#34;
		data-flex-basis=&#34;789px&#34;
	
&gt;
&lt;em&gt;Figure 10: The figure shows an example approach where simplified versions of two tasks (make planks and make sticks) and a model overview of the sub-policies&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As shown in figure 10, the subtasks (ex. $b_1$ which is &amp;ldquo;get wood&amp;rdquo; ) are represented by a unique policy ($\pi_i$); however, the high-level planner can active these components when necessary to make a full plan. In this approach, the high-level model deals with mapping language to sub-policies, and the low-level planners deal with executing action necessary for the sub-task. By using a more hierarchal approach, this approach avoids having to design a single end-to-end network deal with the entire process of converting language to desired actions.&lt;/p&gt;
&lt;h4 id=&#34;advantages--disadvantages-1&#34;&gt;Advantages &amp;amp; Disadvantages&lt;/h4&gt;
&lt;h5 id=&#34;advantages-1&#34;&gt;Advantages&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Generalization:
&lt;ul&gt;
&lt;li&gt;It is possible to recombine existing policies to produce new behaviors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learned task models:
&lt;ul&gt;
&lt;li&gt;Since it uses a learning approach, there is less developmental overhead and can be less brittle than structured language&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Differentiable:
&lt;ul&gt;
&lt;li&gt;Able to learn task execution without needing an expert to specify logical plans&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;disadvantages-1&#34;&gt;Disadvantages&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Brittle:
&lt;ul&gt;
&lt;li&gt;Policies are still triggered with pre-specified words used during training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Plans required:
&lt;ul&gt;
&lt;li&gt;The developers must create new behaviors and know how to combine explicit words to produce behaviors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reward-functions&#34;&gt;Reward Functions&lt;/h3&gt;
&lt;p&gt;One way to reduce interaction time with the environment is to use reward shaping by designing a reward function that provides the agent intermediate rewards for progress towards the goal. Designing a good reward function can be very difficult, expensive, and time-consuming; however, one way to address this problem is by using language instructions to perform reward shaping. This reward function approach mainly involves mapping language to state and action in the world to determine if they are related. This can be done using a framework like LEARN which maps language to rewards based on the agent&amp;rsquo;s actions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/learn.png&#34;
	width=&#34;477&#34;
	height=&#34;400&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/learn_hu88c3dc19022e8ad337c3f270c9cb8f37_97513_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/learn_hu88c3dc19022e8ad337c3f270c9cb8f37_97513_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;286px&#34;
	
&gt;
&lt;em&gt;Figure 11: Example framework which involves training LEARN module and using its intermediate rewards to assist within the agent-environment loop.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Figure 11 shows an example pipeline of producing a reward function from labeled random trajectories. Its main steps involve:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Gather a dataset of labeled trajectories (sequence of past actions)&lt;/li&gt;
&lt;li&gt;Align trajectory information to language commands or prompts
&lt;ul&gt;
&lt;li&gt;Basically, the labelers can explain what they see or say the commands that would have led to that behavior. (&amp;ldquo;Jump over the skull while going left&amp;rdquo;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learn to associate the collected language commands and trajectories (ex. LEARN)&lt;/li&gt;
&lt;li&gt;If the resultant trajectory correctly corresponds to the language sequence, a positive reward is given (Language reward)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;advantages--disadvantages-2&#34;&gt;Advantages &amp;amp; Disadvantages&lt;/h3&gt;
&lt;h4 id=&#34;advantages-2&#34;&gt;Advantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generalization:
&lt;ul&gt;
&lt;li&gt;Learning from human-created language prompts.&lt;/li&gt;
&lt;li&gt;Provide new utterances and map them to new trajectories/plans.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Single model:
&lt;ul&gt;
&lt;li&gt;Do not need to activate relevant policies&lt;/li&gt;
&lt;li&gt;One agent deals with learning the entire mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Grounding is implicit:
&lt;ul&gt;
&lt;li&gt;Do not need to manually label the environment or actions&lt;/li&gt;
&lt;li&gt;The model automatically learns the relationship between words to actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;disadvantages-2&#34;&gt;Disadvantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data collection:
&lt;ul&gt;
&lt;li&gt;Have to collect a LOT of labeled data for learning&lt;/li&gt;
&lt;li&gt;It is also difficult for a human to guide the training afterward&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training time:
&lt;ul&gt;
&lt;li&gt;Whenever new behavior with language needs to be added, the model needs to be fine-tuned before the behavior can be produced.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Single model:
&lt;ul&gt;
&lt;li&gt;Learning for multiple, unrelated tasks might be extremely difficult&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;high-level-language-task-alignment&#34;&gt;High-level language-task alignment&lt;/h2&gt;
&lt;h3 id=&#34;what-are-high-level-tasks&#34;&gt;What are High-level Tasks?&lt;/h3&gt;
&lt;p&gt;In high-level language-task alignment, we focus on giving the agent more complex and &amp;ldquo;abstract&amp;rdquo; instructions, such as &amp;ldquo;go to the library&amp;rdquo;, compared to policy sketches which would use commands such as &amp;ldquo;go forwards, go left, go forwards, go right&amp;rdquo; where we have to explicitly lay out the subtasks for the agent. Note that such high-level tasks usually consist of a sequence of simple actions (known as primitives) or these actions associated in some hierarchical manner. Unlike the methods above, the agent, here, has to learn subtasks that need to be performed to complete the high-level task and has to learn how to complete each subtask.&lt;/p&gt;
&lt;h3 id=&#34;imitating-interactive-intelligence&#34;&gt;Imitating Interactive Intelligence&lt;/h3&gt;
&lt;p&gt;We look at Imitating Interactive Intelligence, a paper published by DeepMind in 2020 to better understand how agents can learn high-level language-task alignment. 
The paper involved two agents in a simulated bedroom as the environment that tried to learn from the demonstrations provided:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one that would provide tasks to be performed (known as the setter)
&lt;ul&gt;
&lt;li&gt;The setter would create a language command based on the interaction and prompt provided by the environment.&lt;/li&gt;
&lt;li&gt;The interaction was the type of general task to be performed (ex: Instruction, Q&amp;amp;A).&lt;/li&gt;
&lt;li&gt;The prompt was used by the environment to specify the type of interaction to be performed (ex: for Q&amp;amp;A, the prompt specified whether the question should ask the count, color, location, or position of an object)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;one that would perform the task created by the setter (known as the solver).
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/iii_relationships.png&#34;
	width=&#34;714&#34;
	height=&#34;445&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/iii_relationships_huc823c259c96ce942146f3257b925ade2_100400_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/iii_relationships_huc823c259c96ce942146f3257b925ade2_100400_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;385px&#34;
	
&gt;
&lt;em&gt;Figure 12: Relationship between Interactions, Prompts, and Instructions Created&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-are-the-agents-trained&#34;&gt;How are the agents trained?&lt;/h3&gt;
&lt;h4 id=&#34;general-process-of-training&#34;&gt;General Process of Training&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Gather human demonstrations&lt;/li&gt;
&lt;li&gt;Apply Behavior Cloning to demonstrations and learn policy from learned reward model through Forward RL to create agent policies&lt;/li&gt;
&lt;li&gt;Apply GAIL to create a reward model from the policy created and demonstrations provided&lt;/li&gt;
&lt;li&gt;Repeat steps 2. and 3. until the solver can complete tasks that are created by the setter.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;using-behavior-cloning-bc-to-create-policies&#34;&gt;Using Behavior Cloning (BC) to Create Policies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;First, the authors took the image data from the simulated environment and passed it through a ResNet (a popular CNN architecture to understand images) to create the image embeddings.&lt;/li&gt;
&lt;li&gt;Similarly, the authors took text data from the prompts provided by the environment, previous language policies generated, and communication between the agents and they tokenized it to create textual embeddings.&lt;/li&gt;
&lt;li&gt;Next, the authors combined these embeddings and passed them through a multimodal transformer, as transformers have been proven to work well for vision-and-language tasks (ex: VILBERT, CLIP, etc).&lt;/li&gt;
&lt;li&gt;The outputs of the transformer would be lastly passed to an LSTM which would produce an autoregressive sequence of actions (meaning that actions depend on previous actions) that are needed to be performed to complete the task as well as any words to output.&lt;/li&gt;
&lt;li&gt;These policies are trained to match the demonstrator&amp;rsquo;s policies via Behavior Cloning. 
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/bc_model.png&#34;
	width=&#34;1157&#34;
	height=&#34;406&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/bc_model_hu8b42d655e5dfc3771f5b339d4d302fca_139511_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/bc_model_hu8b42d655e5dfc3771f5b339d4d302fca_139511_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;284&#34;
		data-flex-basis=&#34;683px&#34;
	
&gt;
&lt;em&gt;Figure 13: Model Architecture for using BC to Create Policies from Image and Language Data&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;using-inverse-reinforcement-learning-irl-to-create-reward-model&#34;&gt;Using Inverse Reinforcement Learning (IRL) to create Reward Model&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Note that the Reward model similarly receives its inputs to the Policy model above: by using image and text data, converting these modalities of data into embeddings, fusing these embeddings, and using a multimodal transformer to understand the fused embeddings.&lt;/li&gt;
&lt;li&gt;The discriminator loss is used similarly as GAIL: to determine whether the trajectories were produced by the agent or if they were sampled from the demonstrations produced by experts.&lt;/li&gt;
&lt;li&gt;As GAIL is difficult to use with high-dimensional data, the authors also use multiple strategies outlined below to make GAIL effective for the setup chosen such as random image augmentation (ex: cropping, rotating, translating images), language matching, and object-in-view.&lt;/li&gt;
&lt;li&gt;The authors employed language matching as an auxiliary task for representation learning that checks whether the instruction created by the setter and the images inputted come from the same episode or not. This would require the ResNet to produce embeddings that have a high degree of mutual information with the tokenized inputs.&lt;/li&gt;
&lt;li&gt;The authors employed object-in-view as another auxiliary task for representation learning that asks the agent to predict whether a certain object is in the image data provided to the agent or not. This requires the ResNet to produce embeddings that reflect the objects present in the image data.
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/irl_model.png&#34;
	width=&#34;1082&#34;
	height=&#34;325&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/irl_model_hu56056958688c966440392f206e5612ab_68793_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/irl_model_hu56056958688c966440392f206e5612ab_68793_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;332&#34;
		data-flex-basis=&#34;799px&#34;
	
&gt;
&lt;em&gt;Figure 14: Model Architecture for using IRL to Create Reward Models from Image and Language Data&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In study A below, the authors found that using behavior cloning and GAIL was more effective than simply learning behavior cloning and not learning the reward model for tasks that required following instructions. However, using GAIL to learn a reward model did not seem to significantly boost performance for question-answering tasks.
&lt;ul&gt;
&lt;li&gt;This is because GAIL can help the agent understand what a task truly means, which is helpful for instruction following, while behavior cloning can not.&lt;/li&gt;
&lt;li&gt;On the other hand, behavior cloning can simply mimic the mapping from questions to answers that the demonstrator uses so it tends to be as effective as using behavior cloning and GAIL for Q&amp;amp;A tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In study B below, the authors also found that for a given task, such as finding the count of a certain object in a room, using data for other tasks for training, such as finding the position/color of an object, led to better performance over limiting the training data to only include demonstrations that directly corresponded to the task that was to be performed.
&lt;ul&gt;
&lt;li&gt;This is because more diverse data and seemingly unrelated commands allow tasks to inform each other (due to some crossover), allows the agent to better understand the language of the commands it receives, and how this language is grounded in the environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Study C shows that this phenomenon also helps with data efficiency; when trained with all types of prompts, the model only needed 1/8th as much data as training with only the &amp;ldquo;Position&amp;rdquo; prompt to reach the same level of performance.&lt;/li&gt;
&lt;li&gt;Study D similarly shows object-color generalization capabilities as the model was able to sufficiently perform tasks dealing with orange ducks such as &amp;ldquo;lift an orange duck&amp;rdquo; even when all instances of orange ducks were removed from the dataset.
&lt;img src=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/iii_studies.png&#34;
	width=&#34;770&#34;
	height=&#34;640&#34;
	srcset=&#34;http://deidnani.github.io/p/leveraging-natural-language/images/iii_studies_hu92f176062201b8cdf5138191578bd086_110739_480x0_resize_box_3.png 480w, http://deidnani.github.io/p/leveraging-natural-language/images/iii_studies_hu92f176062201b8cdf5138191578bd086_110739_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;288px&#34;
	
&gt;
&lt;em&gt;Figure 15: Graphs for some of the studies performed by the authors (described in detail above)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantages--disadvantages-3&#34;&gt;Advantages &amp;amp; Disadvantages&lt;/h3&gt;
&lt;h4 id=&#34;advantages-3&#34;&gt;Advantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generalization:
&lt;ul&gt;
&lt;li&gt;Language commands learned are general and compositional&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Large Multi-Task Model:
&lt;ul&gt;
&lt;li&gt;Agents do not need to learn sub-policies.&lt;/li&gt;
&lt;li&gt;They can simply learn a set of behaviors that can be used for a variety of tasks&lt;/li&gt;
&lt;li&gt;These behaviors also inform each other, leading to better performance for any task when compared to using a single-task approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Production of Language:
&lt;ul&gt;
&lt;li&gt;Agents are not only able to understand the language commands but are also able to produce them&lt;/li&gt;
&lt;li&gt;Such an ability may allow the agent to explain itself&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;disadvantages-3&#34;&gt;Disadvantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data Collection and Complex Training Strategies:
&lt;ul&gt;
&lt;li&gt;This paper required 2+ years of data collection in a complex simulator that required 2 demonstrators interacting.&lt;/li&gt;
&lt;li&gt;Due to the expensiveness of the data collection, this paper is rather difficult to reproduce.&lt;/li&gt;
&lt;li&gt;Also, the complex tricks required may mean that the results are limited to the environment that the authors have set up and the constrained vocabulary (~550 unique words) that they are using&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Independent Training:
&lt;ul&gt;
&lt;li&gt;Agent cannot actively ask for help on new problems.&lt;/li&gt;
&lt;li&gt;For example, the agent cannot ask to see more data on tasks that it has not been asked to perform before or on objects that it has not seen before&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cannot Quickly Acquire New Knowledge:
&lt;ul&gt;
&lt;li&gt;Since many data points are required and a high amount of compute is necessary for training, it is difficult for the model to learn new tasks or to gain new insights quickly, as this would require more data points for those tasks and the model would have to be re-trained from scratch.&lt;/li&gt;
&lt;li&gt;In other words, it is &amp;ldquo;set&amp;rdquo;.
Future Research Directions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Methods to learn from language that is representative of experiences in the &amp;ldquo;real world&amp;rdquo;, rather than using language that is crawled from the Internet
&lt;ul&gt;
&lt;li&gt;Ground language in task execution and objects in the environment and use language is taken directly from humans, rather than just using tweets from Twitter or posts on Reddit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning to use language for Active Learning
&lt;ul&gt;
&lt;li&gt;Creating capabilities for the agent to ask for assistance, clarification, or to remove ambiguities using language, rather than just using offline learning that often requires many data points&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Creating efficient systems of NLP + RL
&lt;ul&gt;
&lt;li&gt;Creating a model that can efficiently learn new tasks and knowledge beyond its existing knowledge base in an efficient manner&lt;/li&gt;
&lt;li&gt;Also, agents must learn to think and act quickly to be used in real-world situations where language instructions are given constantly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning interactively
&lt;ul&gt;
&lt;li&gt;Creating capabilities for the agent to ask for more demonstrations on a certain task or other forms of assistance so that the agent is more involved in the learning process (more like &amp;ldquo;situated learning&amp;rdquo;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Using Audio Data
&lt;ul&gt;
&lt;li&gt;Audio data has previously not been used due to its high dimensionality.&lt;/li&gt;
&lt;li&gt;However, acoustics can influence the meaning of commands; for example, giving a command in a sarcastic tone is quite different from giving it in an urgent tone&lt;/li&gt;
&lt;li&gt;Also, the volume and duration of the acoustics could indicate whether the agent is receiving the command or whether the command was intended for someone else (allows the agent to learn from noisy commands, which typically occurs in the real world)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Using language can help solve agents solve many tasks but it remains difficult due to the problem of grounding the input.&lt;/li&gt;
&lt;li&gt;There are several representations we can use for language such as sequences of tokens, dictionary indices, Word2Vec, GloVe, RNNs, LSTMs, and Transformers, with each representation having its advantages and disadvantages.&lt;/li&gt;
&lt;li&gt;To unify task-learning and language for LfD, there are three main approaches we discussed: structure language approaches, low-level language-task alignment, and high-level language-task alignment (note that each approach is more complex yet more applicable in the real world than the previous).&lt;/li&gt;
&lt;li&gt;Leveraging natural language is still an open area of research with current approaches trying to leverage audio or trying to make these approaches more interactive, efficient, and applicable in the real world.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
